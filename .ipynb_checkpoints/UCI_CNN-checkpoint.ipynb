{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11657df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-07 02:11:41.222442: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import BatchNormalization, Conv1D, Dense, Dropout, Flatten, Layer, Reshape, MaxPooling1D\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d4f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def normalized_labels(model,data):\n",
    "    pred=model.predict(data)\n",
    "    preds2 = deepcopy(pred)\n",
    "    for i in range(pred.shape[0]):\n",
    "        arm = np.argmax(pred[i, :])\n",
    "        preds2[i, :] = 0\n",
    "        preds2[i, arm] = 1\n",
    "    preds2 = preds2.astype('int')\n",
    "    return preds2\n",
    "\n",
    "def int_labels(model,data):\n",
    "    preds2 = normalized_labels(model,data)\n",
    "    preds_encoded = np.argmax(preds2, axis=1).reshape((-1, 1))\n",
    "    return preds_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df03d7",
   "metadata": {},
   "source": [
    "## Loading the UCI dataset - raw sensors data\n",
    "This comprises the complete accelerometer and gyroscope sensor data gathered from an Android mobile device. The dataset was divided into training and testing subsets as follows:\n",
    "- The training set consisted of data from 21 participants.\n",
    "- The testing set consisted of data from 9 participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11a89ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 128, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_train = './UCI HAR Dataset/train/'\n",
    "with open(path_train + 'Inertial Signals/total_acc_x_train.txt', encoding=\"utf8\") as x_acc_train,\\\n",
    "     open(path_train + 'Inertial Signals/total_acc_y_train.txt', encoding=\"utf8\") as y_acc_train,\\\n",
    "     open(path_train + 'Inertial Signals/total_acc_z_train.txt', encoding=\"utf8\") as z_acc_train,\\\n",
    "     open(path_train + 'Inertial Signals/body_gyro_x_train.txt', encoding=\"utf8\") as x_gyr_train,\\\n",
    "     open(path_train + 'Inertial Signals/body_gyro_y_train.txt', encoding=\"utf8\") as y_gyr_train,\\\n",
    "     open(path_train + 'Inertial Signals/body_gyro_z_train.txt', encoding=\"utf8\") as z_gyr_train:\n",
    "    x_acc_train = np.loadtxt(x_acc_train)\n",
    "    y_acc_train = np.loadtxt(y_acc_train)\n",
    "    z_acc_train = np.loadtxt(z_acc_train)\n",
    "    x_gyr_train = np.loadtxt(x_gyr_train)\n",
    "    y_gyr_train = np.loadtxt(y_gyr_train)\n",
    "    z_gyr_train = np.loadtxt(z_gyr_train)\n",
    "    \n",
    "signals_train = np.stack([x_acc_train, y_acc_train, z_acc_train, x_gyr_train, y_gyr_train, z_gyr_train], axis=2)\n",
    "signals_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29b8ce3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2947, 128, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_test = './UCI HAR Dataset/test/'\n",
    "with open(path_test + 'Inertial Signals/total_acc_x_test.txt', encoding=\"utf8\") as x_acc_test,\\\n",
    "     open(path_test + 'Inertial Signals/total_acc_y_test.txt', encoding=\"utf8\") as y_acc_test,\\\n",
    "     open(path_test + 'Inertial Signals/total_acc_z_test.txt', encoding=\"utf8\") as z_acc_test,\\\n",
    "     open(path_test + 'Inertial Signals/body_gyro_x_test.txt', encoding=\"utf8\") as x_gyr_test,\\\n",
    "     open(path_test + 'Inertial Signals/body_gyro_y_test.txt', encoding=\"utf8\") as y_gyr_test,\\\n",
    "     open(path_test + 'Inertial Signals/body_gyro_z_test.txt', encoding=\"utf8\") as z_gyr_test:\n",
    "    x_acc_test = np.loadtxt(x_acc_test)\n",
    "    y_acc_test = np.loadtxt(y_acc_test)\n",
    "    z_acc_test = np.loadtxt(z_acc_test)\n",
    "    x_gyr_test = np.loadtxt(x_gyr_test)\n",
    "    y_gyr_test = np.loadtxt(y_gyr_test)\n",
    "    z_gyr_test = np.loadtxt(z_gyr_test)\n",
    "    \n",
    "signals_test = np.stack([x_acc_test, y_acc_test, z_acc_test, x_gyr_test, y_gyr_test, z_gyr_test], axis=2)\n",
    "signals_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9847037",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./UCI HAR Dataset/train/y_train.txt', encoding=\"utf8\") as act_train,\\\n",
    "     open('./UCI HAR Dataset/test/y_test.txt', encoding=\"utf8\") as act_test:\n",
    "    act_train = np.loadtxt(act_train)\n",
    "    act_test = np.loadtxt(act_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e27e6d8",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Even though the labels data is numeric, it should still be normalized using the One-Hot Encoding technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b66bf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.0: 0, 2.0: 1, 3.0: 2, 4.0: 3, 5.0: 4, 6.0: 5}\n",
      "******************** train data: (7352, 128, 6) ********************\n",
      "******************** train labels: (7352, 6) ********************\n",
      "******************** test data: (2947, 128, 6) ********************\n",
      "******************** test labels: (2947, 6) ********************\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "act_train_normalized = le.fit(act_train)\n",
    "act_train_normalized = le.transform(act_train)\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)\n",
    "\n",
    "act_train_encoding = tf.keras.utils.to_categorical(act_train_normalized, num_classes=6)\n",
    "\n",
    "data_train = signals_train\n",
    "labels_train = act_train_encoding\n",
    "\n",
    "data_test = signals_test\n",
    "labels_test = le.transform(act_test)\n",
    "labels_test = tf.keras.utils.to_categorical(labels_test, num_classes=6)\n",
    "\n",
    "print(\"*\" *20,\"train data:\", data_train.shape,\"*\"*20)\n",
    "print(\"*\" *20,\"train labels:\", labels_train.shape,\"*\"*20)\n",
    "print(\"*\" *20,\"test data:\", data_test.shape,\"*\"*20)\n",
    "print(\"*\" *20,\"test labels:\", labels_test.shape,\"*\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f41c8f",
   "metadata": {},
   "source": [
    "## Building 1D CNN model\n",
    " - Specify certain parameters for the models based on the characteristics of the dataset.\n",
    " - Define an 1D CNN model with range of possible values for all hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "378d7da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_signals = 6 # 3-axis accelerometer and 3-axis gyroscope\n",
    "n_steps = 128 # time window size\n",
    "n_classes = 6 # numbers of activities\n",
    "\n",
    "input_shape = (n_steps,n_classes)\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5144470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    \n",
    "    inputs = tf.keras.Input(shape = input_shape)\n",
    "    cnn1 = tf.keras.layers.Conv1D(filters = hp.Choice('conv_filter_1', values = [64,128,256]),\n",
    "                                  kernel_size = hp.Choice('conv_kernel_1', values = [3,5]), activation='relu')(inputs)\n",
    "    mp_1 = tf.keras.layers.MaxPooling1D(pool_size=2)(cnn1)\n",
    "    do_1 = tf.keras.layers.Dropout(rate = hp.Choice('dropout_1', values = [0.2,0.4,0.6,0.8]))(mp_1)\n",
    "    cnn2 = tf.keras.layers.Conv1D(filters = hp.Choice('conv_filter_2', values = [32,64,128]),\n",
    "                                  kernel_size = hp.Choice('conv_kernel_2', values = [3,5]), activation='relu')(do_1)\n",
    "    mp_2 = tf.keras.layers.MaxPooling1D(pool_size=2)(cnn2)\n",
    "    do_2 = tf.keras.layers.Dropout(rate = hp.Choice('dropout_2', values = [0.2,0.4,0.6,0.8]))(mp_2)\n",
    "    cnn3 = tf.keras.layers.Conv1D(filters = hp.Choice('conv_filter_3', values = [16,32,64]),\n",
    "                                  kernel_size = hp.Choice('conv_kernel_3', values = [3,5]), activation='relu')(do_2)\n",
    "    mp_3 = tf.keras.layers.MaxPooling1D(pool_size=2)(cnn3)\n",
    "    do_3 = tf.keras.layers.Dropout(rate = hp.Choice('dropout_3', values = [0.2,0.4,0.6,0.8]))(mp_3)\n",
    "    cnn4 = tf.keras.layers.Conv1D(filters = hp.Choice('conv_filter_4', values = [16,32]),\n",
    "                                  kernel_size = hp.Choice('conv_kernel_4', values = [3,5]), activation='relu')(do_3)\n",
    "    mp_4 = tf.keras.layers.MaxPooling1D(pool_size=2)(cnn4)\n",
    "    do_4 = tf.keras.layers.Dropout(rate = hp.Choice('dropout_4', values = [0.2,0.4,0.6,0.8]))(mp_3)\n",
    "    ft = tf.keras.layers.Flatten()(do_4)\n",
    "    dense_1 = tf.keras.layers.Dense(units = hp.Choice('dense_units', values = [16,32,64]),\n",
    "                                    activation = 'relu')(ft)\n",
    "    \n",
    "    outputs = Dense(units = n_classes, activation = 'softmax')(dense_1)\n",
    "    model = tf.keras.Model(inputs = inputs, outputs = outputs, name = 'CNN3')\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                  optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3])), \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8a094",
   "metadata": {},
   "source": [
    "- Employ Hyperband optimization to discover the optimal set of hyperparameters.\n",
    "- Assess the model's performance by utilizing the validation accuracy (val_accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc6fd66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 254 Complete [00h 04m 13s]\n",
      "val_accuracy: 0.6879673600196838\n",
      "\n",
      "Best val_accuracy So Far: 0.9456152319908142\n",
      "Total elapsed time: 02h 03m 23s\n",
      "{'conv_filter_1': 256, 'conv_kernel_1': 3, 'dropout_1': 0.6, 'conv_filter_2': 32, 'conv_kernel_2': 3, 'dropout_2': 0.8, 'conv_filter_3': 16, 'conv_kernel_3': 5, 'dropout_3': 0.2, 'conv_filter_4': 16, 'conv_kernel_4': 3, 'dropout_4': 0.4, 'dense_units': 32, 'learning_rate': 0.001, 'tuner/epochs': 100, 'tuner/initial_epoch': 34, 'tuner/bracket': 2, 'tuner/round': 2, 'tuner/trial_id': '0232'}\n",
      "Model: \"CNN3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 6)]          0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 126, 256)          4864      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 63, 256)           0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 63, 256)           0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 61, 32)            24608     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 30, 32)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 30, 32)            0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 26, 16)            2576      \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 13, 16)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 13, 16)            0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 208)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                6688      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 198       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38934 (152.09 KB)\n",
      "Trainable params: 38934 (152.09 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from kerastuner import Hyperband\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "my_dir = './Saved_Models/CNN/'\n",
    "\n",
    "tuner_search=Hyperband(build_model, \n",
    "                        objective='val_accuracy', \n",
    "#                         max_trials=5, \n",
    "                        directory='my_dir',\n",
    "                        project_name='UCI_CNN',\n",
    "                        overwrite = True)\n",
    "tuner_search.search(data_train, labels_train, epochs=3, validation_split=0.2)\n",
    "\n",
    "best_hp = tuner_search.get_best_hyperparameters()[0]\n",
    "print(best_hp.values)\n",
    "\n",
    "model_CNN = tuner_search.get_best_models(num_models=1)[0]\n",
    "#summary of best model\n",
    "model_CNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d46e505",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "- Train the model using a 5-fold cross-validation approach and save the best model\n",
    "- Evaluate the best model performance using test sets\n",
    "- Print out classification report and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "940eae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "******************** Fitting the model ******************** \n",
      "\n",
      "Epoch 1/100\n",
      "365/367 [============================>.] - ETA: 0s - loss: 0.1295 - accuracy: 0.9485\n",
      "Epoch 1: val_loss improved from inf to 0.13557, saving model to /storage1/althome/hanguyen/HAR/UCI/Scripts/Saved_Models/CNN/best_CNN.hdf5\n",
      "367/367 [==============================] - 6s 13ms/step - loss: 0.1293 - accuracy: 0.9486 - val_loss: 0.1356 - val_accuracy: 0.9402\n",
      "Epoch 2/100\n",
      "  1/367 [..............................] - ETA: 47s - loss: 0.5095 - accuracy: 0.8889"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanguyen/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - ETA: 0s - loss: 0.1165 - accuracy: 0.9517\n",
      "Epoch 2: val_loss improved from 0.13557 to 0.09916, saving model to /storage1/althome/hanguyen/HAR/UCI/Scripts/Saved_Models/CNN/best_CNN.hdf5\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.1165 - accuracy: 0.9517 - val_loss: 0.0992 - val_accuracy: 0.9551\n",
      "Epoch 3/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1270 - accuracy: 0.9502\n",
      "Epoch 3: val_loss did not improve from 0.09916\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1270 - accuracy: 0.9502 - val_loss: 0.1058 - val_accuracy: 0.9606\n",
      "Epoch 4/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0972 - accuracy: 0.9565\n",
      "Epoch 4: val_loss improved from 0.09916 to 0.09396, saving model to /storage1/althome/hanguyen/HAR/UCI/Scripts/Saved_Models/CNN/best_CNN.hdf5\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0972 - accuracy: 0.9565 - val_loss: 0.0940 - val_accuracy: 0.9585\n",
      "Epoch 5/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0994 - accuracy: 0.9566\n",
      "Epoch 5: val_loss did not improve from 0.09396\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0997 - accuracy: 0.9562 - val_loss: 0.0999 - val_accuracy: 0.9599\n",
      "Epoch 6/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1008 - accuracy: 0.9564\n",
      "Epoch 6: val_loss did not improve from 0.09396\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1008 - accuracy: 0.9564 - val_loss: 0.0979 - val_accuracy: 0.9565\n",
      "Epoch 7/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0949 - accuracy: 0.9597\n",
      "Epoch 7: val_loss did not improve from 0.09396\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0950 - accuracy: 0.9596 - val_loss: 0.0994 - val_accuracy: 0.9511\n",
      "Epoch 8/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0963 - accuracy: 0.9583\n",
      "Epoch 8: val_loss did not improve from 0.09396\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0968 - accuracy: 0.9581 - val_loss: 0.0971 - val_accuracy: 0.9674\n",
      "Epoch 9/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9526\n",
      "Epoch 9: val_loss did not improve from 0.09396\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1112 - accuracy: 0.9526 - val_loss: 0.1242 - val_accuracy: 0.9592\n",
      "Epoch 10/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.1119 - accuracy: 0.9576\n",
      "Epoch 10: val_loss did not improve from 0.09396\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.1111 - accuracy: 0.9581 - val_loss: 0.1007 - val_accuracy: 0.9579\n",
      "Epoch 11/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1003 - accuracy: 0.9571\n",
      "Epoch 11: val_loss did not improve from 0.09396\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.1002 - accuracy: 0.9570 - val_loss: 0.1226 - val_accuracy: 0.9579\n",
      "Epoch 12/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0933 - accuracy: 0.9579\n",
      "Epoch 12: val_loss improved from 0.09396 to 0.08519, saving model to /storage1/althome/hanguyen/HAR/UCI/Scripts/Saved_Models/CNN/best_CNN.hdf5\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0940 - accuracy: 0.9577 - val_loss: 0.0852 - val_accuracy: 0.9606\n",
      "Epoch 13/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9574\n",
      "Epoch 13: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.1014 - accuracy: 0.9574 - val_loss: 0.1082 - val_accuracy: 0.9538\n",
      "Epoch 14/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9577\n",
      "Epoch 14: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1092 - accuracy: 0.9577 - val_loss: 0.0943 - val_accuracy: 0.9558\n",
      "Epoch 15/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0911 - accuracy: 0.9598\n",
      "Epoch 15: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0912 - accuracy: 0.9598 - val_loss: 0.0860 - val_accuracy: 0.9687\n",
      "Epoch 16/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0927 - accuracy: 0.9586\n",
      "Epoch 16: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0927 - accuracy: 0.9586 - val_loss: 0.0956 - val_accuracy: 0.9531\n",
      "Epoch 17/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0954 - accuracy: 0.9591\n",
      "Epoch 17: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0954 - accuracy: 0.9591 - val_loss: 0.0957 - val_accuracy: 0.9497\n",
      "Epoch 18/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1004 - accuracy: 0.9569\n",
      "Epoch 18: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1004 - accuracy: 0.9569 - val_loss: 0.1078 - val_accuracy: 0.9565\n",
      "Epoch 19/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9574\n",
      "Epoch 19: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1236 - accuracy: 0.9572 - val_loss: 0.1057 - val_accuracy: 0.9565\n",
      "Epoch 20/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0907 - accuracy: 0.9619\n",
      "Epoch 20: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0905 - accuracy: 0.9620 - val_loss: 0.1114 - val_accuracy: 0.9538\n",
      "Epoch 21/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0915 - accuracy: 0.9586\n",
      "Epoch 21: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0915 - accuracy: 0.9586 - val_loss: 0.0934 - val_accuracy: 0.9585\n",
      "Epoch 22/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.0922 - accuracy: 0.9586\n",
      "Epoch 22: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0918 - accuracy: 0.9587 - val_loss: 0.1135 - val_accuracy: 0.9579\n",
      "Epoch 23/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0932 - accuracy: 0.9612\n",
      "Epoch 23: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0938 - accuracy: 0.9613 - val_loss: 0.1051 - val_accuracy: 0.9646\n",
      "Epoch 24/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0886 - accuracy: 0.9636\n",
      "Epoch 24: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0883 - accuracy: 0.9639 - val_loss: 0.1003 - val_accuracy: 0.9687\n",
      "Epoch 25/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0969 - accuracy: 0.9620\n",
      "Epoch 25: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0969 - accuracy: 0.9620 - val_loss: 0.0916 - val_accuracy: 0.9653\n",
      "Epoch 26/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9586\n",
      "Epoch 26: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1001 - accuracy: 0.9586 - val_loss: 0.1119 - val_accuracy: 0.9517\n",
      "Epoch 27/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0890 - accuracy: 0.9622\n",
      "Epoch 27: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0884 - accuracy: 0.9627 - val_loss: 0.0877 - val_accuracy: 0.9660\n",
      "Epoch 28/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0817 - accuracy: 0.9588\n",
      "Epoch 28: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0812 - accuracy: 0.9591 - val_loss: 0.0933 - val_accuracy: 0.9653\n",
      "Epoch 29/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9582\n",
      "Epoch 29: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0875 - accuracy: 0.9582 - val_loss: 0.1095 - val_accuracy: 0.9558\n",
      "Epoch 30/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0901 - accuracy: 0.9609\n",
      "Epoch 30: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0901 - accuracy: 0.9606 - val_loss: 0.0915 - val_accuracy: 0.9572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0964 - accuracy: 0.9602\n",
      "Epoch 31: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0962 - accuracy: 0.9603 - val_loss: 0.1138 - val_accuracy: 0.9626\n",
      "Epoch 32/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0955 - accuracy: 0.9591\n",
      "Epoch 32: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0962 - accuracy: 0.9589 - val_loss: 0.0996 - val_accuracy: 0.9646\n",
      "Epoch 33/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.9552\n",
      "Epoch 33: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0947 - accuracy: 0.9552 - val_loss: 0.1113 - val_accuracy: 0.9640\n",
      "Epoch 34/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1017 - accuracy: 0.9569\n",
      "Epoch 34: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.1017 - accuracy: 0.9569 - val_loss: 0.0859 - val_accuracy: 0.9646\n",
      "Epoch 35/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0935 - accuracy: 0.9606\n",
      "Epoch 35: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0935 - accuracy: 0.9606 - val_loss: 0.0927 - val_accuracy: 0.9646\n",
      "Epoch 36/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.0852 - accuracy: 0.9639\n",
      "Epoch 36: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0855 - accuracy: 0.9635 - val_loss: 0.1017 - val_accuracy: 0.9572\n",
      "Epoch 37/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0884 - accuracy: 0.9586\n",
      "Epoch 37: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0884 - accuracy: 0.9586 - val_loss: 0.0861 - val_accuracy: 0.9613\n",
      "Epoch 38/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0965 - accuracy: 0.9591\n",
      "Epoch 38: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0967 - accuracy: 0.9589 - val_loss: 0.0983 - val_accuracy: 0.9721\n",
      "Epoch 39/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.0816 - accuracy: 0.9624\n",
      "Epoch 39: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0815 - accuracy: 0.9625 - val_loss: 0.0990 - val_accuracy: 0.9660\n",
      "Epoch 40/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0829 - accuracy: 0.9626\n",
      "Epoch 40: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0832 - accuracy: 0.9621 - val_loss: 0.0906 - val_accuracy: 0.9558\n",
      "Epoch 41/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0865 - accuracy: 0.9591\n",
      "Epoch 41: val_loss did not improve from 0.08519\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0865 - accuracy: 0.9591 - val_loss: 0.0900 - val_accuracy: 0.9592\n",
      "Epoch 42/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9644\n",
      "Epoch 42: val_loss improved from 0.08519 to 0.07887, saving model to /storage1/althome/hanguyen/HAR/UCI/Scripts/Saved_Models/CNN/best_CNN.hdf5\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0863 - accuracy: 0.9645 - val_loss: 0.0789 - val_accuracy: 0.9674\n",
      "Epoch 43/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0881 - accuracy: 0.9584\n",
      "Epoch 43: val_loss did not improve from 0.07887\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0881 - accuracy: 0.9584 - val_loss: 0.0801 - val_accuracy: 0.9701\n",
      "Epoch 44/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9627\n",
      "Epoch 44: val_loss did not improve from 0.07887\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0835 - accuracy: 0.9627 - val_loss: 0.0864 - val_accuracy: 0.9708\n",
      "Epoch 45/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9639\n",
      "Epoch 45: val_loss did not improve from 0.07887\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0850 - accuracy: 0.9639 - val_loss: 0.0833 - val_accuracy: 0.9742\n",
      "Epoch 46/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9662\n",
      "Epoch 46: val_loss did not improve from 0.07887\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0739 - accuracy: 0.9662 - val_loss: 0.0833 - val_accuracy: 0.9674\n",
      "Epoch 47/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0954 - accuracy: 0.9597\n",
      "Epoch 47: val_loss did not improve from 0.07887\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0952 - accuracy: 0.9598 - val_loss: 0.0872 - val_accuracy: 0.9633\n",
      "Epoch 48/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9650\n",
      "Epoch 48: val_loss did not improve from 0.07887\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0769 - accuracy: 0.9650 - val_loss: 0.0864 - val_accuracy: 0.9599\n",
      "Epoch 49/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0796 - accuracy: 0.9640\n",
      "Epoch 49: val_loss did not improve from 0.07887\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0796 - accuracy: 0.9640 - val_loss: 0.0828 - val_accuracy: 0.9667\n",
      "Epoch 50/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9650\n",
      "Epoch 50: val_loss improved from 0.07887 to 0.07451, saving model to /storage1/althome/hanguyen/HAR/UCI/Scripts/Saved_Models/CNN/best_CNN.hdf5\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0825 - accuracy: 0.9650 - val_loss: 0.0745 - val_accuracy: 0.9742\n",
      "Epoch 51/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1142 - accuracy: 0.9654\n",
      "Epoch 51: val_loss did not improve from 0.07451\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.1142 - accuracy: 0.9654 - val_loss: 0.0903 - val_accuracy: 0.9646\n",
      "Epoch 52/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0956 - accuracy: 0.9594\n",
      "Epoch 52: val_loss did not improve from 0.07451\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0956 - accuracy: 0.9594 - val_loss: 0.0819 - val_accuracy: 0.9674\n",
      "Epoch 53/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0922 - accuracy: 0.9627\n",
      "Epoch 53: val_loss did not improve from 0.07451\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0922 - accuracy: 0.9627 - val_loss: 0.0756 - val_accuracy: 0.9687\n",
      "Epoch 54/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0864 - accuracy: 0.9633\n",
      "Epoch 54: val_loss did not improve from 0.07451\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0857 - accuracy: 0.9637 - val_loss: 0.0754 - val_accuracy: 0.9714\n",
      "Epoch 55/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0910 - accuracy: 0.9628\n",
      "Epoch 55: val_loss did not improve from 0.07451\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0910 - accuracy: 0.9628 - val_loss: 0.0941 - val_accuracy: 0.9653\n",
      "Epoch 56/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9641\n",
      "Epoch 56: val_loss improved from 0.07451 to 0.07414, saving model to /storage1/althome/hanguyen/HAR/UCI/Scripts/Saved_Models/CNN/best_CNN.hdf5\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0749 - accuracy: 0.9642 - val_loss: 0.0741 - val_accuracy: 0.9701\n",
      "Epoch 57/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9686\n",
      "Epoch 57: val_loss did not improve from 0.07414\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0681 - accuracy: 0.9686 - val_loss: 0.0771 - val_accuracy: 0.9674\n",
      "Epoch 58/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9623\n",
      "Epoch 58: val_loss did not improve from 0.07414\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0810 - accuracy: 0.9623 - val_loss: 0.0753 - val_accuracy: 0.9633\n",
      "Epoch 59/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9625\n",
      "Epoch 59: val_loss did not improve from 0.07414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0816 - accuracy: 0.9625 - val_loss: 0.0788 - val_accuracy: 0.9646\n",
      "Epoch 60/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.1014 - accuracy: 0.9578\n",
      "Epoch 60: val_loss did not improve from 0.07414\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.1007 - accuracy: 0.9579 - val_loss: 0.0806 - val_accuracy: 0.9674\n",
      "Epoch 61/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0681 - accuracy: 0.9677\n",
      "Epoch 61: val_loss did not improve from 0.07414\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0683 - accuracy: 0.9678 - val_loss: 0.0838 - val_accuracy: 0.9613\n",
      "Epoch 62/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9628\n",
      "Epoch 62: val_loss did not improve from 0.07414\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0801 - accuracy: 0.9628 - val_loss: 0.0758 - val_accuracy: 0.9613\n",
      "Epoch 63/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9657\n",
      "Epoch 63: val_loss did not improve from 0.07414\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0834 - accuracy: 0.9657 - val_loss: 0.0747 - val_accuracy: 0.9748\n",
      "Epoch 64/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.0752 - accuracy: 0.9646\n",
      "Epoch 64: val_loss improved from 0.07414 to 0.07034, saving model to /storage1/althome/hanguyen/HAR/UCI/Scripts/Saved_Models/CNN/best_CNN.hdf5\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0747 - accuracy: 0.9649 - val_loss: 0.0703 - val_accuracy: 0.9701\n",
      "Epoch 65/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9650\n",
      "Epoch 65: val_loss did not improve from 0.07034\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0748 - accuracy: 0.9649 - val_loss: 0.0739 - val_accuracy: 0.9667\n",
      "Epoch 66/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9604\n",
      "Epoch 66: val_loss did not improve from 0.07034\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0934 - accuracy: 0.9604 - val_loss: 0.0876 - val_accuracy: 0.9606\n",
      "Epoch 67/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9637\n",
      "Epoch 67: val_loss did not improve from 0.07034\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0774 - accuracy: 0.9637 - val_loss: 0.0710 - val_accuracy: 0.9714\n",
      "Epoch 68/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0937 - accuracy: 0.9593\n",
      "Epoch 68: val_loss did not improve from 0.07034\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0932 - accuracy: 0.9596 - val_loss: 0.0829 - val_accuracy: 0.9619\n",
      "Epoch 69/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0870 - accuracy: 0.9615\n",
      "Epoch 69: val_loss did not improve from 0.07034\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0870 - accuracy: 0.9615 - val_loss: 0.0730 - val_accuracy: 0.9755\n",
      "Epoch 70/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1121 - accuracy: 0.9623\n",
      "Epoch 70: val_loss did not improve from 0.07034\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1121 - accuracy: 0.9623 - val_loss: 0.0903 - val_accuracy: 0.9687\n",
      "Epoch 71/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9685\n",
      "Epoch 71: val_loss improved from 0.07034 to 0.06996, saving model to /storage1/althome/hanguyen/HAR/UCI/Scripts/Saved_Models/CNN/best_CNN.hdf5\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0769 - accuracy: 0.9685 - val_loss: 0.0700 - val_accuracy: 0.9701\n",
      "Epoch 72/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0806 - accuracy: 0.9644\n",
      "Epoch 72: val_loss did not improve from 0.06996\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0806 - accuracy: 0.9644 - val_loss: 0.0732 - val_accuracy: 0.9728\n",
      "Epoch 73/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9665\n",
      "Epoch 73: val_loss did not improve from 0.06996\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0727 - accuracy: 0.9666 - val_loss: 0.0738 - val_accuracy: 0.9714\n",
      "Epoch 74/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9626\n",
      "Epoch 74: val_loss did not improve from 0.06996\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0849 - accuracy: 0.9625 - val_loss: 0.0700 - val_accuracy: 0.9714\n",
      "Epoch 75/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0793 - accuracy: 0.9658\n",
      "Epoch 75: val_loss did not improve from 0.06996\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0797 - accuracy: 0.9657 - val_loss: 0.0735 - val_accuracy: 0.9687\n",
      "Epoch 76/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0776 - accuracy: 0.9634\n",
      "Epoch 76: val_loss improved from 0.06996 to 0.06912, saving model to /storage1/althome/hanguyen/HAR/UCI/Scripts/Saved_Models/CNN/best_CNN.hdf5\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0778 - accuracy: 0.9633 - val_loss: 0.0691 - val_accuracy: 0.9721\n",
      "Epoch 77/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9674\n",
      "Epoch 77: val_loss did not improve from 0.06912\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0726 - accuracy: 0.9674 - val_loss: 0.0814 - val_accuracy: 0.9606\n",
      "Epoch 78/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9685\n",
      "Epoch 78: val_loss did not improve from 0.06912\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0786 - accuracy: 0.9685 - val_loss: 0.0833 - val_accuracy: 0.9653\n",
      "Epoch 79/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0898 - accuracy: 0.9615\n",
      "Epoch 79: val_loss did not improve from 0.06912\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0899 - accuracy: 0.9615 - val_loss: 0.0725 - val_accuracy: 0.9721\n",
      "Epoch 80/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0793 - accuracy: 0.9612\n",
      "Epoch 80: val_loss did not improve from 0.06912\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0791 - accuracy: 0.9613 - val_loss: 0.0693 - val_accuracy: 0.9721\n",
      "Epoch 81/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0816 - accuracy: 0.9646\n",
      "Epoch 81: val_loss did not improve from 0.06912\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0814 - accuracy: 0.9647 - val_loss: 0.0711 - val_accuracy: 0.9728\n",
      "Epoch 82/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9661\n",
      "Epoch 82: val_loss did not improve from 0.06912\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0834 - accuracy: 0.9661 - val_loss: 0.0749 - val_accuracy: 0.9714\n",
      "Epoch 83/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0899 - accuracy: 0.9630\n",
      "Epoch 83: val_loss did not improve from 0.06912\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0899 - accuracy: 0.9630 - val_loss: 0.0754 - val_accuracy: 0.9687\n",
      "Epoch 84/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9649\n",
      "Epoch 84: val_loss improved from 0.06912 to 0.06522, saving model to /storage1/althome/hanguyen/HAR/UCI/Scripts/Saved_Models/CNN/best_CNN.hdf5\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0782 - accuracy: 0.9649 - val_loss: 0.0652 - val_accuracy: 0.9735\n",
      "Epoch 85/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9685\n",
      "Epoch 85: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0700 - accuracy: 0.9685 - val_loss: 0.0654 - val_accuracy: 0.9769\n",
      "Epoch 86/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9661\n",
      "Epoch 86: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0919 - accuracy: 0.9661 - val_loss: 0.0802 - val_accuracy: 0.9653\n",
      "Epoch 87/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0824 - accuracy: 0.9645\n",
      "Epoch 87: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0824 - accuracy: 0.9645 - val_loss: 0.0768 - val_accuracy: 0.9721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9665\n",
      "Epoch 88: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0744 - accuracy: 0.9666 - val_loss: 0.0804 - val_accuracy: 0.9640\n",
      "Epoch 89/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9656\n",
      "Epoch 89: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0798 - accuracy: 0.9656 - val_loss: 0.0745 - val_accuracy: 0.9660\n",
      "Epoch 90/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9650\n",
      "Epoch 90: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0786 - accuracy: 0.9650 - val_loss: 0.0709 - val_accuracy: 0.9748\n",
      "Epoch 91/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0905 - accuracy: 0.9601\n",
      "Epoch 91: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0905 - accuracy: 0.9601 - val_loss: 0.0726 - val_accuracy: 0.9694\n",
      "Epoch 92/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9685\n",
      "Epoch 92: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0693 - accuracy: 0.9685 - val_loss: 0.0715 - val_accuracy: 0.9667\n",
      "Epoch 93/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0794 - accuracy: 0.9656\n",
      "Epoch 93: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0792 - accuracy: 0.9657 - val_loss: 0.0687 - val_accuracy: 0.9735\n",
      "Epoch 94/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0962 - accuracy: 0.9595\n",
      "Epoch 94: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0967 - accuracy: 0.9594 - val_loss: 0.0657 - val_accuracy: 0.9755\n",
      "Epoch 95/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9644\n",
      "Epoch 95: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0784 - accuracy: 0.9644 - val_loss: 0.0811 - val_accuracy: 0.9640\n",
      "Epoch 96/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9662\n",
      "Epoch 96: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0778 - accuracy: 0.9662 - val_loss: 0.0736 - val_accuracy: 0.9769\n",
      "Epoch 97/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9644\n",
      "Epoch 97: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0805 - accuracy: 0.9644 - val_loss: 0.0657 - val_accuracy: 0.9748\n",
      "Epoch 98/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9654\n",
      "Epoch 98: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0743 - accuracy: 0.9654 - val_loss: 0.0693 - val_accuracy: 0.9755\n",
      "Epoch 99/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9639\n",
      "Epoch 99: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0775 - accuracy: 0.9639 - val_loss: 0.0686 - val_accuracy: 0.9721\n",
      "Epoch 100/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0664 - accuracy: 0.9692\n",
      "Epoch 100: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0663 - accuracy: 0.9693 - val_loss: 0.0680 - val_accuracy: 0.9728\n",
      "******************** Accuracy ******************** \n",
      "\n",
      "46/46 [==============================] - 0s 4ms/step\n",
      "                     0.972807613868117                      \n",
      "\n",
      "******************** classification_report ******************** \n",
      "\n",
      "46/46 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       243\n",
      "           1       1.00      1.00      1.00       217\n",
      "           2       1.00      1.00      1.00       185\n",
      "           3       1.00      0.85      0.92       261\n",
      "           4       0.88      1.00      0.94       303\n",
      "           5       1.00      1.00      1.00       262\n",
      "\n",
      "    accuracy                           0.97      1471\n",
      "   macro avg       0.98      0.97      0.98      1471\n",
      "weighted avg       0.98      0.97      0.97      1471\n",
      "\n",
      "Score for fold 1: loss of 0.0680188313126564; accuracy of 97.28075861930847%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "******************** Fitting the model ******************** \n",
      "\n",
      "Epoch 1/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9521\n",
      "Epoch 1: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 6s 13ms/step - loss: 0.1214 - accuracy: 0.9520 - val_loss: 0.0786 - val_accuracy: 0.9694\n",
      "Epoch 2/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1173 - accuracy: 0.9507\n",
      "Epoch 2: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1173 - accuracy: 0.9507 - val_loss: 0.0825 - val_accuracy: 0.9599\n",
      "Epoch 3/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1127 - accuracy: 0.9500\n",
      "Epoch 3: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1127 - accuracy: 0.9500 - val_loss: 0.0924 - val_accuracy: 0.9619\n",
      "Epoch 4/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1023 - accuracy: 0.9559\n",
      "Epoch 4: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.1023 - accuracy: 0.9558 - val_loss: 0.0782 - val_accuracy: 0.9674\n",
      "Epoch 5/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1011 - accuracy: 0.9576\n",
      "Epoch 5: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.1014 - accuracy: 0.9575 - val_loss: 0.0949 - val_accuracy: 0.9531\n",
      "Epoch 6/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.1355 - accuracy: 0.9483\n",
      "Epoch 6: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1352 - accuracy: 0.9483 - val_loss: 0.1246 - val_accuracy: 0.9579\n",
      "Epoch 7/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1113 - accuracy: 0.9521\n",
      "Epoch 7: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1113 - accuracy: 0.9521 - val_loss: 0.1019 - val_accuracy: 0.9490\n",
      "Epoch 8/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.1156 - accuracy: 0.9509\n",
      "Epoch 8: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1158 - accuracy: 0.9511 - val_loss: 0.0907 - val_accuracy: 0.9633\n",
      "Epoch 9/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1124 - accuracy: 0.9533\n",
      "Epoch 9: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.1126 - accuracy: 0.9533 - val_loss: 0.0919 - val_accuracy: 0.9538\n",
      "Epoch 10/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1258 - accuracy: 0.9531\n",
      "Epoch 10: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.1258 - accuracy: 0.9531 - val_loss: 0.0819 - val_accuracy: 0.9755\n",
      "Epoch 11/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9583\n",
      "Epoch 11: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0959 - accuracy: 0.9584 - val_loss: 0.1115 - val_accuracy: 0.9470\n",
      "Epoch 12/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0992 - accuracy: 0.9569\n",
      "Epoch 12: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0993 - accuracy: 0.9569 - val_loss: 0.0963 - val_accuracy: 0.9538\n",
      "Epoch 13/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0960 - accuracy: 0.9606\n",
      "Epoch 13: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0960 - accuracy: 0.9606 - val_loss: 0.0935 - val_accuracy: 0.9538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0995 - accuracy: 0.9578\n",
      "Epoch 14: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1011 - accuracy: 0.9574 - val_loss: 0.0894 - val_accuracy: 0.9640\n",
      "Epoch 15/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.1153 - accuracy: 0.9517\n",
      "Epoch 15: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.1155 - accuracy: 0.9514 - val_loss: 0.1114 - val_accuracy: 0.9538\n",
      "Epoch 16/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0981 - accuracy: 0.9550\n",
      "Epoch 16: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0978 - accuracy: 0.9552 - val_loss: 0.1000 - val_accuracy: 0.9497\n",
      "Epoch 17/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1008 - accuracy: 0.9553\n",
      "Epoch 17: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1008 - accuracy: 0.9553 - val_loss: 0.0982 - val_accuracy: 0.9524\n",
      "Epoch 18/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1077 - accuracy: 0.9523\n",
      "Epoch 18: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1075 - accuracy: 0.9524 - val_loss: 0.1062 - val_accuracy: 0.9463\n",
      "Epoch 19/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0906 - accuracy: 0.9608\n",
      "Epoch 19: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0906 - accuracy: 0.9608 - val_loss: 0.0836 - val_accuracy: 0.9592\n",
      "Epoch 20/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0932 - accuracy: 0.9612\n",
      "Epoch 20: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0930 - accuracy: 0.9613 - val_loss: 0.0797 - val_accuracy: 0.9708\n",
      "Epoch 21/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0873 - accuracy: 0.9618\n",
      "Epoch 21: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0873 - accuracy: 0.9618 - val_loss: 0.0848 - val_accuracy: 0.9531\n",
      "Epoch 22/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0861 - accuracy: 0.9603\n",
      "Epoch 22: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0861 - accuracy: 0.9603 - val_loss: 0.0813 - val_accuracy: 0.9613\n",
      "Epoch 23/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9591\n",
      "Epoch 23: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0898 - accuracy: 0.9591 - val_loss: 0.0888 - val_accuracy: 0.9545\n",
      "Epoch 24/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9619\n",
      "Epoch 24: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0869 - accuracy: 0.9620 - val_loss: 0.0775 - val_accuracy: 0.9660\n",
      "Epoch 25/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0989 - accuracy: 0.9564\n",
      "Epoch 25: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0989 - accuracy: 0.9564 - val_loss: 0.0834 - val_accuracy: 0.9613\n",
      "Epoch 26/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0915 - accuracy: 0.9574\n",
      "Epoch 26: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0913 - accuracy: 0.9575 - val_loss: 0.0852 - val_accuracy: 0.9613\n",
      "Epoch 27/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9589\n",
      "Epoch 27: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1062 - accuracy: 0.9589 - val_loss: 0.0719 - val_accuracy: 0.9606\n",
      "Epoch 28/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9584\n",
      "Epoch 28: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1042 - accuracy: 0.9584 - val_loss: 0.1082 - val_accuracy: 0.9483\n",
      "Epoch 29/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0907 - accuracy: 0.9601\n",
      "Epoch 29: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0907 - accuracy: 0.9601 - val_loss: 0.0832 - val_accuracy: 0.9667\n",
      "Epoch 30/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0842 - accuracy: 0.9610\n",
      "Epoch 30: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0842 - accuracy: 0.9610 - val_loss: 0.0726 - val_accuracy: 0.9653\n",
      "Epoch 31/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0922 - accuracy: 0.9584\n",
      "Epoch 31: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0922 - accuracy: 0.9584 - val_loss: 0.0776 - val_accuracy: 0.9640\n",
      "Epoch 32/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0906 - accuracy: 0.9591\n",
      "Epoch 32: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0903 - accuracy: 0.9592 - val_loss: 0.0749 - val_accuracy: 0.9721\n",
      "Epoch 33/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0920 - accuracy: 0.9620\n",
      "Epoch 33: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0920 - accuracy: 0.9620 - val_loss: 0.1012 - val_accuracy: 0.9667\n",
      "Epoch 34/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0905 - accuracy: 0.9592\n",
      "Epoch 34: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0905 - accuracy: 0.9592 - val_loss: 0.0770 - val_accuracy: 0.9633\n",
      "Epoch 35/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0859 - accuracy: 0.9624\n",
      "Epoch 35: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0856 - accuracy: 0.9627 - val_loss: 0.0864 - val_accuracy: 0.9585\n",
      "Epoch 36/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0798 - accuracy: 0.9628\n",
      "Epoch 36: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0805 - accuracy: 0.9625 - val_loss: 0.0787 - val_accuracy: 0.9653\n",
      "Epoch 37/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0853 - accuracy: 0.9610\n",
      "Epoch 37: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0853 - accuracy: 0.9610 - val_loss: 0.0862 - val_accuracy: 0.9599\n",
      "Epoch 38/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0986 - accuracy: 0.9589\n",
      "Epoch 38: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0986 - accuracy: 0.9589 - val_loss: 0.0784 - val_accuracy: 0.9640\n",
      "Epoch 39/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0837 - accuracy: 0.9630\n",
      "Epoch 39: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0837 - accuracy: 0.9630 - val_loss: 0.0756 - val_accuracy: 0.9633\n",
      "Epoch 40/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9647\n",
      "Epoch 40: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0826 - accuracy: 0.9647 - val_loss: 0.0708 - val_accuracy: 0.9674\n",
      "Epoch 41/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0832 - accuracy: 0.9627\n",
      "Epoch 41: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0834 - accuracy: 0.9627 - val_loss: 0.0753 - val_accuracy: 0.9667\n",
      "Epoch 42/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0979 - accuracy: 0.9587\n",
      "Epoch 42: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0979 - accuracy: 0.9587 - val_loss: 0.0801 - val_accuracy: 0.9667\n",
      "Epoch 43/100\n",
      "365/367 [============================>.] - ETA: 0s - loss: 0.0847 - accuracy: 0.9607\n",
      "Epoch 43: val_loss did not improve from 0.06522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0845 - accuracy: 0.9610 - val_loss: 0.0709 - val_accuracy: 0.9721\n",
      "Epoch 44/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0872 - accuracy: 0.9613\n",
      "Epoch 44: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0872 - accuracy: 0.9613 - val_loss: 0.0965 - val_accuracy: 0.9606\n",
      "Epoch 45/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9620\n",
      "Epoch 45: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0782 - accuracy: 0.9620 - val_loss: 0.0807 - val_accuracy: 0.9660\n",
      "Epoch 46/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0926 - accuracy: 0.9581\n",
      "Epoch 46: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0926 - accuracy: 0.9581 - val_loss: 0.0798 - val_accuracy: 0.9640\n",
      "Epoch 47/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0889 - accuracy: 0.9616\n",
      "Epoch 47: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0889 - accuracy: 0.9616 - val_loss: 0.1117 - val_accuracy: 0.9415\n",
      "Epoch 48/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0847 - accuracy: 0.9628\n",
      "Epoch 48: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0848 - accuracy: 0.9627 - val_loss: 0.0855 - val_accuracy: 0.9551\n",
      "Epoch 49/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0887 - accuracy: 0.9639\n",
      "Epoch 49: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0887 - accuracy: 0.9639 - val_loss: 0.0839 - val_accuracy: 0.9585\n",
      "Epoch 50/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0844 - accuracy: 0.9633\n",
      "Epoch 50: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0844 - accuracy: 0.9633 - val_loss: 0.0709 - val_accuracy: 0.9646\n",
      "Epoch 51/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9650\n",
      "Epoch 51: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0740 - accuracy: 0.9650 - val_loss: 0.0686 - val_accuracy: 0.9674\n",
      "Epoch 52/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 0.9594\n",
      "Epoch 52: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0983 - accuracy: 0.9594 - val_loss: 0.0794 - val_accuracy: 0.9599\n",
      "Epoch 53/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9650\n",
      "Epoch 53: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0783 - accuracy: 0.9650 - val_loss: 0.0656 - val_accuracy: 0.9687\n",
      "Epoch 54/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0961 - accuracy: 0.9630\n",
      "Epoch 54: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0961 - accuracy: 0.9630 - val_loss: 0.0783 - val_accuracy: 0.9687\n",
      "Epoch 55/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0905 - accuracy: 0.9604\n",
      "Epoch 55: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0905 - accuracy: 0.9604 - val_loss: 0.0767 - val_accuracy: 0.9674\n",
      "Epoch 56/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9642\n",
      "Epoch 56: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0775 - accuracy: 0.9642 - val_loss: 0.0803 - val_accuracy: 0.9579\n",
      "Epoch 57/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0852 - accuracy: 0.9603\n",
      "Epoch 57: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0852 - accuracy: 0.9603 - val_loss: 0.0921 - val_accuracy: 0.9545\n",
      "Epoch 58/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0697 - accuracy: 0.9699\n",
      "Epoch 58: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0697 - accuracy: 0.9700 - val_loss: 0.0705 - val_accuracy: 0.9633\n",
      "Epoch 59/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.9642\n",
      "Epoch 59: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0808 - accuracy: 0.9642 - val_loss: 0.0674 - val_accuracy: 0.9660\n",
      "Epoch 60/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1054 - accuracy: 0.9567\n",
      "Epoch 60: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1054 - accuracy: 0.9567 - val_loss: 0.0797 - val_accuracy: 0.9619\n",
      "Epoch 61/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9642\n",
      "Epoch 61: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0778 - accuracy: 0.9642 - val_loss: 0.0734 - val_accuracy: 0.9626\n",
      "Epoch 62/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9627\n",
      "Epoch 62: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0774 - accuracy: 0.9627 - val_loss: 0.0929 - val_accuracy: 0.9504\n",
      "Epoch 63/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0801 - accuracy: 0.9643\n",
      "Epoch 63: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0800 - accuracy: 0.9642 - val_loss: 0.0754 - val_accuracy: 0.9592\n",
      "Epoch 64/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9647\n",
      "Epoch 64: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0753 - accuracy: 0.9647 - val_loss: 0.0903 - val_accuracy: 0.9456\n",
      "Epoch 65/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0914 - accuracy: 0.9648\n",
      "Epoch 65: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0910 - accuracy: 0.9647 - val_loss: 0.0919 - val_accuracy: 0.9538\n",
      "Epoch 66/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0797 - accuracy: 0.9636\n",
      "Epoch 66: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0801 - accuracy: 0.9633 - val_loss: 0.0853 - val_accuracy: 0.9551\n",
      "Epoch 67/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.0773 - accuracy: 0.9677\n",
      "Epoch 67: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0772 - accuracy: 0.9676 - val_loss: 0.0689 - val_accuracy: 0.9667\n",
      "Epoch 68/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0957 - accuracy: 0.9633\n",
      "Epoch 68: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0957 - accuracy: 0.9633 - val_loss: 0.0708 - val_accuracy: 0.9667\n",
      "Epoch 69/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9635\n",
      "Epoch 69: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0997 - accuracy: 0.9635 - val_loss: 0.0777 - val_accuracy: 0.9633\n",
      "Epoch 70/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0795 - accuracy: 0.9650\n",
      "Epoch 70: val_loss did not improve from 0.06522\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0795 - accuracy: 0.9650 - val_loss: 0.0656 - val_accuracy: 0.9660\n",
      "Epoch 71/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0660 - accuracy: 0.9679\n",
      "Epoch 71: val_loss improved from 0.06522 to 0.05998, saving model to /storage1/althome/hanguyen/HAR/UCI/Scripts/Saved_Models/CNN/best_CNN.hdf5\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0658 - accuracy: 0.9681 - val_loss: 0.0600 - val_accuracy: 0.9721\n",
      "Epoch 72/100\n",
      "  7/367 [..............................] - ETA: 3s - loss: 0.0483 - accuracy: 0.9821"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanguyen/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/367 [============================>.] - ETA: 0s - loss: 0.0695 - accuracy: 0.9702\n",
      "Epoch 72: val_loss did not improve from 0.05998\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0696 - accuracy: 0.9702 - val_loss: 0.0619 - val_accuracy: 0.9680\n",
      "Epoch 73/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0863 - accuracy: 0.9639\n",
      "Epoch 73: val_loss did not improve from 0.05998\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0863 - accuracy: 0.9639 - val_loss: 0.0644 - val_accuracy: 0.9646\n",
      "Epoch 74/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0843 - accuracy: 0.9641\n",
      "Epoch 74: val_loss did not improve from 0.05998\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0853 - accuracy: 0.9639 - val_loss: 0.0816 - val_accuracy: 0.9640\n",
      "Epoch 75/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0905 - accuracy: 0.9600\n",
      "Epoch 75: val_loss did not improve from 0.05998\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0903 - accuracy: 0.9601 - val_loss: 0.0729 - val_accuracy: 0.9674\n",
      "Epoch 76/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9659\n",
      "Epoch 76: val_loss did not improve from 0.05998\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0770 - accuracy: 0.9659 - val_loss: 0.0803 - val_accuracy: 0.9619\n",
      "Epoch 77/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0776 - accuracy: 0.9662\n",
      "Epoch 77: val_loss did not improve from 0.05998\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0780 - accuracy: 0.9659 - val_loss: 0.0728 - val_accuracy: 0.9674\n",
      "Epoch 78/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9669\n",
      "Epoch 78: val_loss did not improve from 0.05998\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0740 - accuracy: 0.9668 - val_loss: 0.0668 - val_accuracy: 0.9694\n",
      "Epoch 79/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.0792 - accuracy: 0.9666\n",
      "Epoch 79: val_loss did not improve from 0.05998\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0796 - accuracy: 0.9661 - val_loss: 0.0695 - val_accuracy: 0.9646\n",
      "Epoch 80/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9668\n",
      "Epoch 80: val_loss improved from 0.05998 to 0.05921, saving model to /storage1/althome/hanguyen/HAR/UCI/Scripts/Saved_Models/CNN/best_CNN.hdf5\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0678 - accuracy: 0.9668 - val_loss: 0.0592 - val_accuracy: 0.9735\n",
      "Epoch 81/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9688\n",
      "Epoch 81: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0698 - accuracy: 0.9688 - val_loss: 0.0623 - val_accuracy: 0.9646\n",
      "Epoch 82/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9668\n",
      "Epoch 82: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0779 - accuracy: 0.9668 - val_loss: 0.0798 - val_accuracy: 0.9619\n",
      "Epoch 83/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9685\n",
      "Epoch 83: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0738 - accuracy: 0.9685 - val_loss: 0.0726 - val_accuracy: 0.9646\n",
      "Epoch 84/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0689 - accuracy: 0.9678\n",
      "Epoch 84: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0689 - accuracy: 0.9679 - val_loss: 0.0761 - val_accuracy: 0.9572\n",
      "Epoch 85/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9693\n",
      "Epoch 85: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0722 - accuracy: 0.9693 - val_loss: 0.0662 - val_accuracy: 0.9653\n",
      "Epoch 86/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9644\n",
      "Epoch 86: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0916 - accuracy: 0.9644 - val_loss: 0.1432 - val_accuracy: 0.9599\n",
      "Epoch 87/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9688\n",
      "Epoch 87: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0783 - accuracy: 0.9688 - val_loss: 0.0694 - val_accuracy: 0.9674\n",
      "Epoch 88/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0763 - accuracy: 0.9685\n",
      "Epoch 88: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0758 - accuracy: 0.9686 - val_loss: 0.0655 - val_accuracy: 0.9667\n",
      "Epoch 89/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9673\n",
      "Epoch 89: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0692 - accuracy: 0.9673 - val_loss: 0.0683 - val_accuracy: 0.9694\n",
      "Epoch 90/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9647\n",
      "Epoch 90: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0744 - accuracy: 0.9647 - val_loss: 0.0649 - val_accuracy: 0.9680\n",
      "Epoch 91/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0808 - accuracy: 0.9665\n",
      "Epoch 91: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0806 - accuracy: 0.9666 - val_loss: 0.0729 - val_accuracy: 0.9626\n",
      "Epoch 92/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0778 - accuracy: 0.9646\n",
      "Epoch 92: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0785 - accuracy: 0.9644 - val_loss: 0.0593 - val_accuracy: 0.9708\n",
      "Epoch 93/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0775 - accuracy: 0.9648\n",
      "Epoch 93: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0779 - accuracy: 0.9649 - val_loss: 0.0685 - val_accuracy: 0.9653\n",
      "Epoch 94/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0683 - accuracy: 0.9686\n",
      "Epoch 94: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0680 - accuracy: 0.9690 - val_loss: 0.0632 - val_accuracy: 0.9708\n",
      "Epoch 95/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9666\n",
      "Epoch 95: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0743 - accuracy: 0.9666 - val_loss: 0.0620 - val_accuracy: 0.9742\n",
      "Epoch 96/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9664\n",
      "Epoch 96: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0762 - accuracy: 0.9664 - val_loss: 0.0776 - val_accuracy: 0.9633\n",
      "Epoch 97/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0909 - accuracy: 0.9638\n",
      "Epoch 97: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0904 - accuracy: 0.9639 - val_loss: 0.0669 - val_accuracy: 0.9653\n",
      "Epoch 98/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9688\n",
      "Epoch 98: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0687 - accuracy: 0.9688 - val_loss: 0.0617 - val_accuracy: 0.9660\n",
      "Epoch 99/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9662\n",
      "Epoch 99: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0775 - accuracy: 0.9662 - val_loss: 0.0677 - val_accuracy: 0.9667\n",
      "Epoch 100/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1102 - accuracy: 0.9641\n",
      "Epoch 100: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1100 - accuracy: 0.9642 - val_loss: 0.0675 - val_accuracy: 0.9660\n",
      "******************** Accuracy ******************** \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 4ms/step\n",
      "                     0.9660095173351462                      \n",
      "\n",
      "******************** classification_report ******************** \n",
      "\n",
      "46/46 [==============================] - 0s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       258\n",
      "           1       1.00      1.00      1.00       203\n",
      "           2       1.00      1.00      1.00       206\n",
      "           3       0.97      0.83      0.89       245\n",
      "           4       0.86      0.97      0.91       259\n",
      "           5       1.00      1.00      1.00       300\n",
      "\n",
      "    accuracy                           0.97      1471\n",
      "   macro avg       0.97      0.97      0.97      1471\n",
      "weighted avg       0.97      0.97      0.97      1471\n",
      "\n",
      "Score for fold 2: loss of 0.06750932335853577; accuracy of 96.60094976425171%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "******************** Fitting the model ******************** \n",
      "\n",
      "Epoch 1/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1240 - accuracy: 0.9489\n",
      "Epoch 1: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 6s 13ms/step - loss: 0.1240 - accuracy: 0.9489 - val_loss: 0.1084 - val_accuracy: 0.9544\n",
      "Epoch 2/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.9523\n",
      "Epoch 2: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.1307 - accuracy: 0.9523 - val_loss: 0.1095 - val_accuracy: 0.9531\n",
      "Epoch 3/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1116 - accuracy: 0.9564\n",
      "Epoch 3: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1115 - accuracy: 0.9565 - val_loss: 0.0957 - val_accuracy: 0.9578\n",
      "Epoch 4/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 0.9565\n",
      "Epoch 4: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0983 - accuracy: 0.9565 - val_loss: 0.0972 - val_accuracy: 0.9463\n",
      "Epoch 5/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1144 - accuracy: 0.9501\n",
      "Epoch 5: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1144 - accuracy: 0.9501 - val_loss: 0.1265 - val_accuracy: 0.9320\n",
      "Epoch 6/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1090 - accuracy: 0.9574\n",
      "Epoch 6: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.1090 - accuracy: 0.9574 - val_loss: 0.1029 - val_accuracy: 0.9646\n",
      "Epoch 7/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1013 - accuracy: 0.9552\n",
      "Epoch 7: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1013 - accuracy: 0.9552 - val_loss: 0.0877 - val_accuracy: 0.9619\n",
      "Epoch 8/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9572\n",
      "Epoch 8: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1014 - accuracy: 0.9572 - val_loss: 0.1042 - val_accuracy: 0.9497\n",
      "Epoch 9/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1037 - accuracy: 0.9540\n",
      "Epoch 9: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1037 - accuracy: 0.9540 - val_loss: 0.0889 - val_accuracy: 0.9633\n",
      "Epoch 10/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1041 - accuracy: 0.9544\n",
      "Epoch 10: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.1038 - accuracy: 0.9545 - val_loss: 0.1007 - val_accuracy: 0.9578\n",
      "Epoch 11/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1069 - accuracy: 0.9555\n",
      "Epoch 11: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1069 - accuracy: 0.9555 - val_loss: 0.1199 - val_accuracy: 0.9449\n",
      "Epoch 12/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0941 - accuracy: 0.9569\n",
      "Epoch 12: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0951 - accuracy: 0.9567 - val_loss: 0.1155 - val_accuracy: 0.9320\n",
      "Epoch 13/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0834 - accuracy: 0.9634\n",
      "Epoch 13: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0836 - accuracy: 0.9632 - val_loss: 0.0835 - val_accuracy: 0.9653\n",
      "Epoch 14/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1036 - accuracy: 0.9547\n",
      "Epoch 14: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1036 - accuracy: 0.9547 - val_loss: 0.0964 - val_accuracy: 0.9619\n",
      "Epoch 15/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0996 - accuracy: 0.9571\n",
      "Epoch 15: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0998 - accuracy: 0.9569 - val_loss: 0.1136 - val_accuracy: 0.9612\n",
      "Epoch 16/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9574\n",
      "Epoch 16: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0934 - accuracy: 0.9574 - val_loss: 0.0931 - val_accuracy: 0.9612\n",
      "Epoch 17/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.1013 - accuracy: 0.9554\n",
      "Epoch 17: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1008 - accuracy: 0.9557 - val_loss: 0.0945 - val_accuracy: 0.9626\n",
      "Epoch 18/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0906 - accuracy: 0.9600\n",
      "Epoch 18: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0905 - accuracy: 0.9601 - val_loss: 0.0995 - val_accuracy: 0.9639\n",
      "Epoch 19/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9590\n",
      "Epoch 19: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0867 - accuracy: 0.9589 - val_loss: 0.0817 - val_accuracy: 0.9612\n",
      "Epoch 20/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9574\n",
      "Epoch 20: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0970 - accuracy: 0.9574 - val_loss: 0.0917 - val_accuracy: 0.9592\n",
      "Epoch 21/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0928 - accuracy: 0.9584\n",
      "Epoch 21: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0928 - accuracy: 0.9584 - val_loss: 0.1026 - val_accuracy: 0.9510\n",
      "Epoch 22/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0896 - accuracy: 0.9601\n",
      "Epoch 22: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0896 - accuracy: 0.9601 - val_loss: 0.0921 - val_accuracy: 0.9612\n",
      "Epoch 23/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0987 - accuracy: 0.9564\n",
      "Epoch 23: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0987 - accuracy: 0.9564 - val_loss: 0.0904 - val_accuracy: 0.9667\n",
      "Epoch 24/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0882 - accuracy: 0.9619\n",
      "Epoch 24: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0877 - accuracy: 0.9620 - val_loss: 0.1022 - val_accuracy: 0.9435\n",
      "Epoch 25/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0947 - accuracy: 0.9593\n",
      "Epoch 25: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0954 - accuracy: 0.9586 - val_loss: 0.0964 - val_accuracy: 0.9551\n",
      "Epoch 26/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0921 - accuracy: 0.9588\n",
      "Epoch 26: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0923 - accuracy: 0.9586 - val_loss: 0.0957 - val_accuracy: 0.9599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0948 - accuracy: 0.9569\n",
      "Epoch 27: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0947 - accuracy: 0.9569 - val_loss: 0.1030 - val_accuracy: 0.9429\n",
      "Epoch 28/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0929 - accuracy: 0.9581\n",
      "Epoch 28: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0929 - accuracy: 0.9581 - val_loss: 0.0863 - val_accuracy: 0.9626\n",
      "Epoch 29/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9625\n",
      "Epoch 29: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0798 - accuracy: 0.9625 - val_loss: 0.0739 - val_accuracy: 0.9701\n",
      "Epoch 30/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0890 - accuracy: 0.9576\n",
      "Epoch 30: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0888 - accuracy: 0.9576 - val_loss: 0.1169 - val_accuracy: 0.9231\n",
      "Epoch 31/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9610\n",
      "Epoch 31: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0919 - accuracy: 0.9610 - val_loss: 0.0956 - val_accuracy: 0.9558\n",
      "Epoch 32/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0826 - accuracy: 0.9660\n",
      "Epoch 32: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0828 - accuracy: 0.9657 - val_loss: 0.0851 - val_accuracy: 0.9612\n",
      "Epoch 33/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0936 - accuracy: 0.9615\n",
      "Epoch 33: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0936 - accuracy: 0.9615 - val_loss: 0.0788 - val_accuracy: 0.9653\n",
      "Epoch 34/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.1066 - accuracy: 0.9577\n",
      "Epoch 34: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1071 - accuracy: 0.9574 - val_loss: 0.1095 - val_accuracy: 0.9415\n",
      "Epoch 35/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0965 - accuracy: 0.9616\n",
      "Epoch 35: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0965 - accuracy: 0.9616 - val_loss: 0.0802 - val_accuracy: 0.9673\n",
      "Epoch 36/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0843 - accuracy: 0.9620\n",
      "Epoch 36: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0843 - accuracy: 0.9620 - val_loss: 0.0811 - val_accuracy: 0.9639\n",
      "Epoch 37/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0796 - accuracy: 0.9648\n",
      "Epoch 37: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0790 - accuracy: 0.9651 - val_loss: 0.0795 - val_accuracy: 0.9646\n",
      "Epoch 38/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9639\n",
      "Epoch 38: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0857 - accuracy: 0.9639 - val_loss: 0.0788 - val_accuracy: 0.9639\n",
      "Epoch 39/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9594\n",
      "Epoch 39: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0919 - accuracy: 0.9594 - val_loss: 0.0934 - val_accuracy: 0.9578\n",
      "Epoch 40/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.1011 - accuracy: 0.9558\n",
      "Epoch 40: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1009 - accuracy: 0.9560 - val_loss: 0.0898 - val_accuracy: 0.9626\n",
      "Epoch 41/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0793 - accuracy: 0.9656\n",
      "Epoch 41: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0793 - accuracy: 0.9656 - val_loss: 0.0818 - val_accuracy: 0.9660\n",
      "Epoch 42/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0839 - accuracy: 0.9618\n",
      "Epoch 42: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0839 - accuracy: 0.9618 - val_loss: 0.0790 - val_accuracy: 0.9694\n",
      "Epoch 43/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.9603\n",
      "Epoch 43: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1183 - accuracy: 0.9603 - val_loss: 0.0837 - val_accuracy: 0.9680\n",
      "Epoch 44/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9618\n",
      "Epoch 44: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0919 - accuracy: 0.9618 - val_loss: 0.0809 - val_accuracy: 0.9653\n",
      "Epoch 45/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9652\n",
      "Epoch 45: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0834 - accuracy: 0.9652 - val_loss: 0.0854 - val_accuracy: 0.9626\n",
      "Epoch 46/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9630\n",
      "Epoch 46: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0803 - accuracy: 0.9630 - val_loss: 0.0856 - val_accuracy: 0.9571\n",
      "Epoch 47/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9616\n",
      "Epoch 47: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0849 - accuracy: 0.9616 - val_loss: 0.1011 - val_accuracy: 0.9483\n",
      "Epoch 48/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0886 - accuracy: 0.9624\n",
      "Epoch 48: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0890 - accuracy: 0.9622 - val_loss: 0.0860 - val_accuracy: 0.9510\n",
      "Epoch 49/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9603\n",
      "Epoch 49: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0812 - accuracy: 0.9603 - val_loss: 0.0816 - val_accuracy: 0.9673\n",
      "Epoch 50/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0877 - accuracy: 0.9616\n",
      "Epoch 50: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0877 - accuracy: 0.9616 - val_loss: 0.0838 - val_accuracy: 0.9639\n",
      "Epoch 51/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0832 - accuracy: 0.9621\n",
      "Epoch 51: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0835 - accuracy: 0.9618 - val_loss: 0.0809 - val_accuracy: 0.9673\n",
      "Epoch 52/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0896 - accuracy: 0.9625\n",
      "Epoch 52: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0896 - accuracy: 0.9625 - val_loss: 0.0895 - val_accuracy: 0.9639\n",
      "Epoch 53/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0846 - accuracy: 0.9623\n",
      "Epoch 53: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0846 - accuracy: 0.9623 - val_loss: 0.0943 - val_accuracy: 0.9524\n",
      "Epoch 54/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9657\n",
      "Epoch 54: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0775 - accuracy: 0.9657 - val_loss: 0.1049 - val_accuracy: 0.9401\n",
      "Epoch 55/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.9623\n",
      "Epoch 55: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0823 - accuracy: 0.9623 - val_loss: 0.0930 - val_accuracy: 0.9537\n",
      "Epoch 56/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0866 - accuracy: 0.9635\n",
      "Epoch 56: val_loss did not improve from 0.05921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0866 - accuracy: 0.9635 - val_loss: 0.0885 - val_accuracy: 0.9531\n",
      "Epoch 57/100\n",
      "365/367 [============================>.] - ETA: 0s - loss: 0.0847 - accuracy: 0.9645\n",
      "Epoch 57: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0852 - accuracy: 0.9644 - val_loss: 0.0795 - val_accuracy: 0.9673\n",
      "Epoch 58/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0882 - accuracy: 0.9626\n",
      "Epoch 58: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0877 - accuracy: 0.9627 - val_loss: 0.0806 - val_accuracy: 0.9687\n",
      "Epoch 59/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9682\n",
      "Epoch 59: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0769 - accuracy: 0.9680 - val_loss: 0.0886 - val_accuracy: 0.9694\n",
      "Epoch 60/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9661\n",
      "Epoch 60: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0770 - accuracy: 0.9661 - val_loss: 0.0761 - val_accuracy: 0.9687\n",
      "Epoch 61/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0833 - accuracy: 0.9625\n",
      "Epoch 61: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0833 - accuracy: 0.9625 - val_loss: 0.0935 - val_accuracy: 0.9578\n",
      "Epoch 62/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0902 - accuracy: 0.9613\n",
      "Epoch 62: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0902 - accuracy: 0.9613 - val_loss: 0.0985 - val_accuracy: 0.9429\n",
      "Epoch 63/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0830 - accuracy: 0.9663\n",
      "Epoch 63: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0828 - accuracy: 0.9664 - val_loss: 0.0759 - val_accuracy: 0.9694\n",
      "Epoch 64/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1099 - accuracy: 0.9528\n",
      "Epoch 64: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1099 - accuracy: 0.9528 - val_loss: 0.0826 - val_accuracy: 0.9633\n",
      "Epoch 65/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0760 - accuracy: 0.9657\n",
      "Epoch 65: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0759 - accuracy: 0.9657 - val_loss: 0.0948 - val_accuracy: 0.9544\n",
      "Epoch 66/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9644\n",
      "Epoch 66: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0804 - accuracy: 0.9644 - val_loss: 0.0785 - val_accuracy: 0.9687\n",
      "Epoch 67/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0922 - accuracy: 0.9632\n",
      "Epoch 67: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0920 - accuracy: 0.9633 - val_loss: 0.0784 - val_accuracy: 0.9673\n",
      "Epoch 68/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0704 - accuracy: 0.9677\n",
      "Epoch 68: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0702 - accuracy: 0.9678 - val_loss: 0.0778 - val_accuracy: 0.9680\n",
      "Epoch 69/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9637\n",
      "Epoch 69: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0832 - accuracy: 0.9637 - val_loss: 0.0798 - val_accuracy: 0.9667\n",
      "Epoch 70/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0882 - accuracy: 0.9649\n",
      "Epoch 70: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0882 - accuracy: 0.9649 - val_loss: 0.0989 - val_accuracy: 0.9619\n",
      "Epoch 71/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0929 - accuracy: 0.9621\n",
      "Epoch 71: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0929 - accuracy: 0.9620 - val_loss: 0.0838 - val_accuracy: 0.9687\n",
      "Epoch 72/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9628\n",
      "Epoch 72: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0971 - accuracy: 0.9628 - val_loss: 0.0978 - val_accuracy: 0.9565\n",
      "Epoch 73/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.9656\n",
      "Epoch 73: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0714 - accuracy: 0.9654 - val_loss: 0.1049 - val_accuracy: 0.9442\n",
      "Epoch 74/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0876 - accuracy: 0.9643\n",
      "Epoch 74: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0881 - accuracy: 0.9640 - val_loss: 0.0841 - val_accuracy: 0.9721\n",
      "Epoch 75/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0792 - accuracy: 0.9655\n",
      "Epoch 75: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0793 - accuracy: 0.9654 - val_loss: 0.1020 - val_accuracy: 0.9463\n",
      "Epoch 76/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9656\n",
      "Epoch 76: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0757 - accuracy: 0.9656 - val_loss: 0.0780 - val_accuracy: 0.9701\n",
      "Epoch 77/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9659\n",
      "Epoch 77: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0769 - accuracy: 0.9659 - val_loss: 0.0857 - val_accuracy: 0.9619\n",
      "Epoch 78/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.1151 - accuracy: 0.9609\n",
      "Epoch 78: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1157 - accuracy: 0.9606 - val_loss: 0.0710 - val_accuracy: 0.9701\n",
      "Epoch 79/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0904 - accuracy: 0.9617\n",
      "Epoch 79: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0906 - accuracy: 0.9615 - val_loss: 0.0878 - val_accuracy: 0.9585\n",
      "Epoch 80/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9657\n",
      "Epoch 80: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0783 - accuracy: 0.9657 - val_loss: 0.0862 - val_accuracy: 0.9667\n",
      "Epoch 81/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9708\n",
      "Epoch 81: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0680 - accuracy: 0.9708 - val_loss: 0.0716 - val_accuracy: 0.9701\n",
      "Epoch 82/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.9687\n",
      "Epoch 82: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0718 - accuracy: 0.9688 - val_loss: 0.0644 - val_accuracy: 0.9694\n",
      "Epoch 83/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9684\n",
      "Epoch 83: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0735 - accuracy: 0.9683 - val_loss: 0.0725 - val_accuracy: 0.9735\n",
      "Epoch 84/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9653\n",
      "Epoch 84: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0844 - accuracy: 0.9654 - val_loss: 0.0721 - val_accuracy: 0.9701\n",
      "Epoch 85/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0790 - accuracy: 0.9675\n",
      "Epoch 85: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0789 - accuracy: 0.9676 - val_loss: 0.0705 - val_accuracy: 0.9728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0657 - accuracy: 0.9686\n",
      "Epoch 86: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0658 - accuracy: 0.9685 - val_loss: 0.0778 - val_accuracy: 0.9646\n",
      "Epoch 87/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9662\n",
      "Epoch 87: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0729 - accuracy: 0.9662 - val_loss: 0.0700 - val_accuracy: 0.9701\n",
      "Epoch 88/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9678\n",
      "Epoch 88: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0710 - accuracy: 0.9678 - val_loss: 0.0758 - val_accuracy: 0.9680\n",
      "Epoch 89/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0794 - accuracy: 0.9674\n",
      "Epoch 89: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0796 - accuracy: 0.9671 - val_loss: 0.0706 - val_accuracy: 0.9694\n",
      "Epoch 90/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0873 - accuracy: 0.9627\n",
      "Epoch 90: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0871 - accuracy: 0.9628 - val_loss: 0.0808 - val_accuracy: 0.9633\n",
      "Epoch 91/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9649\n",
      "Epoch 91: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0820 - accuracy: 0.9649 - val_loss: 0.0744 - val_accuracy: 0.9667\n",
      "Epoch 92/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0712 - accuracy: 0.9641\n",
      "Epoch 92: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0711 - accuracy: 0.9642 - val_loss: 0.0654 - val_accuracy: 0.9721\n",
      "Epoch 93/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9681\n",
      "Epoch 93: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0695 - accuracy: 0.9681 - val_loss: 0.0743 - val_accuracy: 0.9660\n",
      "Epoch 94/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.0816 - accuracy: 0.9625\n",
      "Epoch 94: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0813 - accuracy: 0.9628 - val_loss: 0.0738 - val_accuracy: 0.9673\n",
      "Epoch 95/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.1006 - accuracy: 0.9636\n",
      "Epoch 95: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1008 - accuracy: 0.9633 - val_loss: 0.0759 - val_accuracy: 0.9673\n",
      "Epoch 96/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9710\n",
      "Epoch 96: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0603 - accuracy: 0.9710 - val_loss: 0.0617 - val_accuracy: 0.9721\n",
      "Epoch 97/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9680\n",
      "Epoch 97: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0720 - accuracy: 0.9680 - val_loss: 0.0613 - val_accuracy: 0.9748\n",
      "Epoch 98/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9666\n",
      "Epoch 98: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0762 - accuracy: 0.9666 - val_loss: 0.0741 - val_accuracy: 0.9660\n",
      "Epoch 99/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0973 - accuracy: 0.9657\n",
      "Epoch 99: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1017 - accuracy: 0.9657 - val_loss: 0.0783 - val_accuracy: 0.9660\n",
      "Epoch 100/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9640\n",
      "Epoch 100: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0832 - accuracy: 0.9640 - val_loss: 0.0765 - val_accuracy: 0.9701\n",
      "******************** Accuracy ******************** \n",
      "\n",
      "46/46 [==============================] - 0s 4ms/step\n",
      "                     0.9700680272108844                      \n",
      "\n",
      "******************** classification_report ******************** \n",
      "\n",
      "46/46 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       249\n",
      "           1       1.00      1.00      1.00       207\n",
      "           2       1.00      1.00      1.00       195\n",
      "           3       0.96      0.86      0.91       240\n",
      "           4       0.88      0.97      0.92       278\n",
      "           5       1.00      1.00      1.00       301\n",
      "\n",
      "    accuracy                           0.97      1470\n",
      "   macro avg       0.97      0.97      0.97      1470\n",
      "weighted avg       0.97      0.97      0.97      1470\n",
      "\n",
      "Score for fold 3: loss of 0.0764959380030632; accuracy of 97.00680375099182%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "******************** Fitting the model ******************** \n",
      "\n",
      "Epoch 1/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.1342 - accuracy: 0.9476\n",
      "Epoch 1: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 6s 13ms/step - loss: 0.1335 - accuracy: 0.9481 - val_loss: 0.1190 - val_accuracy: 0.9687\n",
      "Epoch 2/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1158 - accuracy: 0.9461\n",
      "Epoch 2: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.1158 - accuracy: 0.9461 - val_loss: 0.0909 - val_accuracy: 0.9680\n",
      "Epoch 3/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1170 - accuracy: 0.9497\n",
      "Epoch 3: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.1168 - accuracy: 0.9499 - val_loss: 0.1025 - val_accuracy: 0.9408\n",
      "Epoch 4/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 0.9528\n",
      "Epoch 4: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.1173 - accuracy: 0.9529 - val_loss: 0.0867 - val_accuracy: 0.9728\n",
      "Epoch 5/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1133 - accuracy: 0.9523\n",
      "Epoch 5: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.1131 - accuracy: 0.9524 - val_loss: 0.0843 - val_accuracy: 0.9755\n",
      "Epoch 6/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.1170 - accuracy: 0.9488\n",
      "Epoch 6: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1163 - accuracy: 0.9490 - val_loss: 0.1065 - val_accuracy: 0.9367\n",
      "Epoch 7/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 0.9541\n",
      "Epoch 7: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0992 - accuracy: 0.9541 - val_loss: 0.0978 - val_accuracy: 0.9537\n",
      "Epoch 8/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.1103 - accuracy: 0.9559\n",
      "Epoch 8: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1103 - accuracy: 0.9558 - val_loss: 0.1245 - val_accuracy: 0.9184\n",
      "Epoch 9/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.1038 - accuracy: 0.9516\n",
      "Epoch 9: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1035 - accuracy: 0.9519 - val_loss: 0.0874 - val_accuracy: 0.9626\n",
      "Epoch 10/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9581\n",
      "Epoch 10: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0984 - accuracy: 0.9581 - val_loss: 0.0881 - val_accuracy: 0.9639\n",
      "Epoch 11/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.1006 - accuracy: 0.9554\n",
      "Epoch 11: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.1001 - accuracy: 0.9555 - val_loss: 0.0949 - val_accuracy: 0.9639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1041 - accuracy: 0.9536\n",
      "Epoch 12: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1041 - accuracy: 0.9536 - val_loss: 0.0874 - val_accuracy: 0.9565\n",
      "Epoch 13/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0964 - accuracy: 0.9555\n",
      "Epoch 13: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0965 - accuracy: 0.9552 - val_loss: 0.0847 - val_accuracy: 0.9646\n",
      "Epoch 14/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0957 - accuracy: 0.9564\n",
      "Epoch 14: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0955 - accuracy: 0.9565 - val_loss: 0.0917 - val_accuracy: 0.9592\n",
      "Epoch 15/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0922 - accuracy: 0.9545\n",
      "Epoch 15: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0922 - accuracy: 0.9545 - val_loss: 0.0866 - val_accuracy: 0.9571\n",
      "Epoch 16/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0974 - accuracy: 0.9581\n",
      "Epoch 16: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0974 - accuracy: 0.9581 - val_loss: 0.1377 - val_accuracy: 0.9476\n",
      "Epoch 17/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1091 - accuracy: 0.9564\n",
      "Epoch 17: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1091 - accuracy: 0.9564 - val_loss: 0.0773 - val_accuracy: 0.9714\n",
      "Epoch 18/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1002 - accuracy: 0.9552\n",
      "Epoch 18: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1002 - accuracy: 0.9552 - val_loss: 0.0929 - val_accuracy: 0.9517\n",
      "Epoch 19/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0967 - accuracy: 0.9554\n",
      "Epoch 19: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0965 - accuracy: 0.9555 - val_loss: 0.0794 - val_accuracy: 0.9646\n",
      "Epoch 20/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.0992 - accuracy: 0.9551\n",
      "Epoch 20: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0992 - accuracy: 0.9552 - val_loss: 0.0793 - val_accuracy: 0.9735\n",
      "Epoch 21/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0956 - accuracy: 0.9571\n",
      "Epoch 21: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0963 - accuracy: 0.9567 - val_loss: 0.0786 - val_accuracy: 0.9673\n",
      "Epoch 22/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0927 - accuracy: 0.9584\n",
      "Epoch 22: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0927 - accuracy: 0.9584 - val_loss: 0.0891 - val_accuracy: 0.9660\n",
      "Epoch 23/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0924 - accuracy: 0.9589\n",
      "Epoch 23: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0924 - accuracy: 0.9589 - val_loss: 0.0743 - val_accuracy: 0.9646\n",
      "Epoch 24/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.0984 - accuracy: 0.9594\n",
      "Epoch 24: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0997 - accuracy: 0.9589 - val_loss: 0.0849 - val_accuracy: 0.9728\n",
      "Epoch 25/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9576\n",
      "Epoch 25: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0959 - accuracy: 0.9577 - val_loss: 0.0924 - val_accuracy: 0.9408\n",
      "Epoch 26/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0980 - accuracy: 0.9543\n",
      "Epoch 26: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0980 - accuracy: 0.9543 - val_loss: 0.0939 - val_accuracy: 0.9469\n",
      "Epoch 27/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9579\n",
      "Epoch 27: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0921 - accuracy: 0.9579 - val_loss: 0.0730 - val_accuracy: 0.9728\n",
      "Epoch 28/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0975 - accuracy: 0.9564\n",
      "Epoch 28: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0975 - accuracy: 0.9564 - val_loss: 0.0921 - val_accuracy: 0.9612\n",
      "Epoch 29/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0888 - accuracy: 0.9588\n",
      "Epoch 29: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0886 - accuracy: 0.9589 - val_loss: 0.0978 - val_accuracy: 0.9456\n",
      "Epoch 30/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0944 - accuracy: 0.9573\n",
      "Epoch 30: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0946 - accuracy: 0.9570 - val_loss: 0.1111 - val_accuracy: 0.9354\n",
      "Epoch 31/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0957 - accuracy: 0.9565\n",
      "Epoch 31: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0957 - accuracy: 0.9565 - val_loss: 0.1018 - val_accuracy: 0.9735\n",
      "Epoch 32/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1049 - accuracy: 0.9561\n",
      "Epoch 32: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1047 - accuracy: 0.9562 - val_loss: 0.0880 - val_accuracy: 0.9673\n",
      "Epoch 33/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0886 - accuracy: 0.9623\n",
      "Epoch 33: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0887 - accuracy: 0.9622 - val_loss: 0.0898 - val_accuracy: 0.9578\n",
      "Epoch 34/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9605\n",
      "Epoch 34: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1087 - accuracy: 0.9605 - val_loss: 0.0836 - val_accuracy: 0.9544\n",
      "Epoch 35/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1166 - accuracy: 0.9553\n",
      "Epoch 35: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1166 - accuracy: 0.9553 - val_loss: 0.0816 - val_accuracy: 0.9639\n",
      "Epoch 36/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9562\n",
      "Epoch 36: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0967 - accuracy: 0.9562 - val_loss: 0.0763 - val_accuracy: 0.9619\n",
      "Epoch 37/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0948 - accuracy: 0.9589\n",
      "Epoch 37: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0948 - accuracy: 0.9589 - val_loss: 0.0820 - val_accuracy: 0.9524\n",
      "Epoch 38/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0903 - accuracy: 0.9603\n",
      "Epoch 38: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0903 - accuracy: 0.9603 - val_loss: 0.0962 - val_accuracy: 0.9415\n",
      "Epoch 39/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0864 - accuracy: 0.9595\n",
      "Epoch 39: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0864 - accuracy: 0.9594 - val_loss: 0.0796 - val_accuracy: 0.9585\n",
      "Epoch 40/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0881 - accuracy: 0.9623\n",
      "Epoch 40: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0881 - accuracy: 0.9623 - val_loss: 0.0694 - val_accuracy: 0.9694\n",
      "Epoch 41/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0799 - accuracy: 0.9617\n",
      "Epoch 41: val_loss did not improve from 0.05921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0798 - accuracy: 0.9618 - val_loss: 0.0705 - val_accuracy: 0.9748\n",
      "Epoch 42/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0976 - accuracy: 0.9555\n",
      "Epoch 42: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0976 - accuracy: 0.9555 - val_loss: 0.0739 - val_accuracy: 0.9592\n",
      "Epoch 43/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0876 - accuracy: 0.9601\n",
      "Epoch 43: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0876 - accuracy: 0.9601 - val_loss: 0.0793 - val_accuracy: 0.9599\n",
      "Epoch 44/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.1132 - accuracy: 0.9572\n",
      "Epoch 44: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1138 - accuracy: 0.9569 - val_loss: 0.0914 - val_accuracy: 0.9619\n",
      "Epoch 45/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0880 - accuracy: 0.9578\n",
      "Epoch 45: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0879 - accuracy: 0.9577 - val_loss: 0.0778 - val_accuracy: 0.9578\n",
      "Epoch 46/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1041 - accuracy: 0.9587\n",
      "Epoch 46: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1041 - accuracy: 0.9587 - val_loss: 0.1039 - val_accuracy: 0.9456\n",
      "Epoch 47/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0940 - accuracy: 0.9572\n",
      "Epoch 47: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0940 - accuracy: 0.9572 - val_loss: 0.0731 - val_accuracy: 0.9714\n",
      "Epoch 48/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9584\n",
      "Epoch 48: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0875 - accuracy: 0.9584 - val_loss: 0.0722 - val_accuracy: 0.9592\n",
      "Epoch 49/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0844 - accuracy: 0.9651\n",
      "Epoch 49: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0844 - accuracy: 0.9651 - val_loss: 0.0731 - val_accuracy: 0.9735\n",
      "Epoch 50/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0840 - accuracy: 0.9631\n",
      "Epoch 50: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0848 - accuracy: 0.9625 - val_loss: 0.0818 - val_accuracy: 0.9592\n",
      "Epoch 51/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0831 - accuracy: 0.9616\n",
      "Epoch 51: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0831 - accuracy: 0.9616 - val_loss: 0.0687 - val_accuracy: 0.9741\n",
      "Epoch 52/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0982 - accuracy: 0.9598\n",
      "Epoch 52: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0977 - accuracy: 0.9599 - val_loss: 0.0683 - val_accuracy: 0.9721\n",
      "Epoch 53/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0836 - accuracy: 0.9622\n",
      "Epoch 53: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0836 - accuracy: 0.9622 - val_loss: 0.0883 - val_accuracy: 0.9578\n",
      "Epoch 54/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0831 - accuracy: 0.9587\n",
      "Epoch 54: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0831 - accuracy: 0.9587 - val_loss: 0.0684 - val_accuracy: 0.9673\n",
      "Epoch 55/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9609\n",
      "Epoch 55: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.1065 - accuracy: 0.9608 - val_loss: 0.1045 - val_accuracy: 0.9463\n",
      "Epoch 56/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0893 - accuracy: 0.9589\n",
      "Epoch 56: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0893 - accuracy: 0.9589 - val_loss: 0.0779 - val_accuracy: 0.9660\n",
      "Epoch 57/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9642\n",
      "Epoch 57: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0816 - accuracy: 0.9642 - val_loss: 0.0691 - val_accuracy: 0.9653\n",
      "Epoch 58/100\n",
      "365/367 [============================>.] - ETA: 0s - loss: 0.0852 - accuracy: 0.9630\n",
      "Epoch 58: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0850 - accuracy: 0.9632 - val_loss: 0.0770 - val_accuracy: 0.9735\n",
      "Epoch 59/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0841 - accuracy: 0.9639\n",
      "Epoch 59: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0841 - accuracy: 0.9639 - val_loss: 0.0720 - val_accuracy: 0.9639\n",
      "Epoch 60/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0951 - accuracy: 0.9618\n",
      "Epoch 60: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0951 - accuracy: 0.9618 - val_loss: 0.1011 - val_accuracy: 0.9497\n",
      "Epoch 61/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9581\n",
      "Epoch 61: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0869 - accuracy: 0.9581 - val_loss: 0.0764 - val_accuracy: 0.9633\n",
      "Epoch 62/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9664\n",
      "Epoch 62: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0765 - accuracy: 0.9664 - val_loss: 0.0707 - val_accuracy: 0.9646\n",
      "Epoch 63/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0958 - accuracy: 0.9618\n",
      "Epoch 63: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0958 - accuracy: 0.9618 - val_loss: 0.0669 - val_accuracy: 0.9701\n",
      "Epoch 64/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.0801 - accuracy: 0.9630\n",
      "Epoch 64: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0803 - accuracy: 0.9630 - val_loss: 0.0960 - val_accuracy: 0.9531\n",
      "Epoch 65/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0792 - accuracy: 0.9654\n",
      "Epoch 65: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0792 - accuracy: 0.9654 - val_loss: 0.0763 - val_accuracy: 0.9653\n",
      "Epoch 66/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0836 - accuracy: 0.9616\n",
      "Epoch 66: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0836 - accuracy: 0.9616 - val_loss: 0.0668 - val_accuracy: 0.9748\n",
      "Epoch 67/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0868 - accuracy: 0.9627\n",
      "Epoch 67: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0868 - accuracy: 0.9627 - val_loss: 0.0675 - val_accuracy: 0.9728\n",
      "Epoch 68/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.0890 - accuracy: 0.9620\n",
      "Epoch 68: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0891 - accuracy: 0.9618 - val_loss: 0.0757 - val_accuracy: 0.9592\n",
      "Epoch 69/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0905 - accuracy: 0.9645\n",
      "Epoch 69: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0905 - accuracy: 0.9645 - val_loss: 0.0822 - val_accuracy: 0.9612\n",
      "Epoch 70/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0795 - accuracy: 0.9662\n",
      "Epoch 70: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0795 - accuracy: 0.9662 - val_loss: 0.0739 - val_accuracy: 0.9660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0804 - accuracy: 0.9638\n",
      "Epoch 71: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0806 - accuracy: 0.9635 - val_loss: 0.0698 - val_accuracy: 0.9748\n",
      "Epoch 72/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0978 - accuracy: 0.9625\n",
      "Epoch 72: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0978 - accuracy: 0.9625 - val_loss: 0.0812 - val_accuracy: 0.9646\n",
      "Epoch 73/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9669\n",
      "Epoch 73: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0826 - accuracy: 0.9669 - val_loss: 0.0682 - val_accuracy: 0.9782\n",
      "Epoch 74/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9645\n",
      "Epoch 74: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0780 - accuracy: 0.9645 - val_loss: 0.0754 - val_accuracy: 0.9714\n",
      "Epoch 75/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0826 - accuracy: 0.9632\n",
      "Epoch 75: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0824 - accuracy: 0.9633 - val_loss: 0.0812 - val_accuracy: 0.9619\n",
      "Epoch 76/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0923 - accuracy: 0.9614\n",
      "Epoch 76: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0922 - accuracy: 0.9615 - val_loss: 0.0700 - val_accuracy: 0.9612\n",
      "Epoch 77/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0942 - accuracy: 0.9662\n",
      "Epoch 77: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0942 - accuracy: 0.9662 - val_loss: 0.0653 - val_accuracy: 0.9769\n",
      "Epoch 78/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.9615\n",
      "Epoch 78: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0823 - accuracy: 0.9615 - val_loss: 0.0756 - val_accuracy: 0.9707\n",
      "Epoch 79/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9627\n",
      "Epoch 79: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0866 - accuracy: 0.9627 - val_loss: 0.0634 - val_accuracy: 0.9728\n",
      "Epoch 80/100\n",
      "364/367 [============================>.] - ETA: 0s - loss: 0.0818 - accuracy: 0.9641\n",
      "Epoch 80: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0813 - accuracy: 0.9642 - val_loss: 0.0746 - val_accuracy: 0.9585\n",
      "Epoch 81/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9708\n",
      "Epoch 81: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0727 - accuracy: 0.9708 - val_loss: 0.0749 - val_accuracy: 0.9701\n",
      "Epoch 82/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1271 - accuracy: 0.9611\n",
      "Epoch 82: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1271 - accuracy: 0.9611 - val_loss: 0.0709 - val_accuracy: 0.9660\n",
      "Epoch 83/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0844 - accuracy: 0.9632\n",
      "Epoch 83: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0849 - accuracy: 0.9630 - val_loss: 0.0736 - val_accuracy: 0.9571\n",
      "Epoch 84/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 0.9668\n",
      "Epoch 84: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0744 - accuracy: 0.9669 - val_loss: 0.0762 - val_accuracy: 0.9673\n",
      "Epoch 85/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0887 - accuracy: 0.9655\n",
      "Epoch 85: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0889 - accuracy: 0.9654 - val_loss: 0.0622 - val_accuracy: 0.9694\n",
      "Epoch 86/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.9649\n",
      "Epoch 86: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0807 - accuracy: 0.9649 - val_loss: 0.0687 - val_accuracy: 0.9782\n",
      "Epoch 87/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0817 - accuracy: 0.9626\n",
      "Epoch 87: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0813 - accuracy: 0.9628 - val_loss: 0.0716 - val_accuracy: 0.9633\n",
      "Epoch 88/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9695\n",
      "Epoch 88: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0713 - accuracy: 0.9695 - val_loss: 0.0750 - val_accuracy: 0.9646\n",
      "Epoch 89/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0864 - accuracy: 0.9651\n",
      "Epoch 89: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0864 - accuracy: 0.9651 - val_loss: 0.0600 - val_accuracy: 0.9776\n",
      "Epoch 90/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9648\n",
      "Epoch 90: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0756 - accuracy: 0.9647 - val_loss: 0.0659 - val_accuracy: 0.9660\n",
      "Epoch 91/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0777 - accuracy: 0.9674\n",
      "Epoch 91: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0776 - accuracy: 0.9674 - val_loss: 0.0764 - val_accuracy: 0.9680\n",
      "Epoch 92/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.9656\n",
      "Epoch 92: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0829 - accuracy: 0.9656 - val_loss: 0.0926 - val_accuracy: 0.9503\n",
      "Epoch 93/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0860 - accuracy: 0.9629\n",
      "Epoch 93: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0861 - accuracy: 0.9628 - val_loss: 0.0662 - val_accuracy: 0.9707\n",
      "Epoch 94/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9635\n",
      "Epoch 94: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0804 - accuracy: 0.9635 - val_loss: 0.0734 - val_accuracy: 0.9680\n",
      "Epoch 95/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9668\n",
      "Epoch 95: val_loss did not improve from 0.05921\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0722 - accuracy: 0.9666 - val_loss: 0.0605 - val_accuracy: 0.9755\n",
      "Epoch 96/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0860 - accuracy: 0.9643\n",
      "Epoch 96: val_loss improved from 0.05921 to 0.05822, saving model to /storage1/althome/hanguyen/HAR/UCI/Scripts/Saved_Models/CNN/best_CNN.hdf5\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0861 - accuracy: 0.9642 - val_loss: 0.0582 - val_accuracy: 0.9755\n",
      "Epoch 97/100\n",
      " 12/367 [..............................] - ETA: 3s - loss: 0.1021 - accuracy: 0.9531"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanguyen/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9659\n",
      "Epoch 97: val_loss did not improve from 0.05822\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0762 - accuracy: 0.9659 - val_loss: 0.0753 - val_accuracy: 0.9605\n",
      "Epoch 98/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0853 - accuracy: 0.9633\n",
      "Epoch 98: val_loss did not improve from 0.05822\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0848 - accuracy: 0.9637 - val_loss: 0.0664 - val_accuracy: 0.9680\n",
      "Epoch 99/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9680\n",
      "Epoch 99: val_loss improved from 0.05822 to 0.05761, saving model to /storage1/althome/hanguyen/HAR/UCI/Scripts/Saved_Models/CNN/best_CNN.hdf5\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0735 - accuracy: 0.9680 - val_loss: 0.0576 - val_accuracy: 0.9735\n",
      "Epoch 100/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9682\n",
      "Epoch 100: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0725 - accuracy: 0.9683 - val_loss: 0.0614 - val_accuracy: 0.9769\n",
      "******************** Accuracy ******************** \n",
      "\n",
      "46/46 [==============================] - 0s 3ms/step\n",
      "                     0.9768707482993197                      \n",
      "\n",
      "******************** classification_report ******************** \n",
      "\n",
      "46/46 [==============================] - 0s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       247\n",
      "           1       1.00      1.00      1.00       211\n",
      "           2       1.00      1.00      1.00       197\n",
      "           3       0.96      0.90      0.93       257\n",
      "           4       0.91      0.97      0.94       263\n",
      "           5       1.00      1.00      1.00       295\n",
      "\n",
      "    accuracy                           0.98      1470\n",
      "   macro avg       0.98      0.98      0.98      1470\n",
      "weighted avg       0.98      0.98      0.98      1470\n",
      "\n",
      "Score for fold 4: loss of 0.06135591119527817; accuracy of 97.68707752227783%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "******************** Fitting the model ******************** \n",
      "\n",
      "Epoch 1/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1410 - accuracy: 0.9471\n",
      "Epoch 1: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 6s 13ms/step - loss: 0.1409 - accuracy: 0.9470 - val_loss: 0.0942 - val_accuracy: 0.9673\n",
      "Epoch 2/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1162 - accuracy: 0.9508\n",
      "Epoch 2: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.1160 - accuracy: 0.9509 - val_loss: 0.1151 - val_accuracy: 0.9293\n",
      "Epoch 3/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1090 - accuracy: 0.9530\n",
      "Epoch 3: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.1105 - accuracy: 0.9524 - val_loss: 0.1051 - val_accuracy: 0.9483\n",
      "Epoch 4/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1228 - accuracy: 0.9547\n",
      "Epoch 4: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.1228 - accuracy: 0.9547 - val_loss: 0.1132 - val_accuracy: 0.9639\n",
      "Epoch 5/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.1097 - accuracy: 0.9550\n",
      "Epoch 5: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1098 - accuracy: 0.9548 - val_loss: 0.0904 - val_accuracy: 0.9646\n",
      "Epoch 6/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0995 - accuracy: 0.9566\n",
      "Epoch 6: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1002 - accuracy: 0.9565 - val_loss: 0.0925 - val_accuracy: 0.9639\n",
      "Epoch 7/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0996 - accuracy: 0.9552\n",
      "Epoch 7: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0996 - accuracy: 0.9552 - val_loss: 0.1148 - val_accuracy: 0.9503\n",
      "Epoch 8/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0983 - accuracy: 0.9536\n",
      "Epoch 8: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0982 - accuracy: 0.9538 - val_loss: 0.0966 - val_accuracy: 0.9551\n",
      "Epoch 9/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0982 - accuracy: 0.9579\n",
      "Epoch 9: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0982 - accuracy: 0.9579 - val_loss: 0.1126 - val_accuracy: 0.9456\n",
      "Epoch 10/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1005 - accuracy: 0.9586\n",
      "Epoch 10: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1005 - accuracy: 0.9586 - val_loss: 0.0979 - val_accuracy: 0.9565\n",
      "Epoch 11/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1015 - accuracy: 0.9594\n",
      "Epoch 11: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.1015 - accuracy: 0.9594 - val_loss: 0.0903 - val_accuracy: 0.9612\n",
      "Epoch 12/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0896 - accuracy: 0.9633\n",
      "Epoch 12: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0893 - accuracy: 0.9635 - val_loss: 0.0887 - val_accuracy: 0.9680\n",
      "Epoch 13/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.1259 - accuracy: 0.9537\n",
      "Epoch 13: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1256 - accuracy: 0.9538 - val_loss: 0.1009 - val_accuracy: 0.9646\n",
      "Epoch 14/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0865 - accuracy: 0.9595\n",
      "Epoch 14: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0864 - accuracy: 0.9594 - val_loss: 0.1088 - val_accuracy: 0.9476\n",
      "Epoch 15/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0952 - accuracy: 0.9601\n",
      "Epoch 15: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0952 - accuracy: 0.9601 - val_loss: 0.0841 - val_accuracy: 0.9680\n",
      "Epoch 16/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0982 - accuracy: 0.9564\n",
      "Epoch 16: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0982 - accuracy: 0.9564 - val_loss: 0.1066 - val_accuracy: 0.9469\n",
      "Epoch 17/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0959 - accuracy: 0.9558\n",
      "Epoch 17: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0959 - accuracy: 0.9558 - val_loss: 0.0882 - val_accuracy: 0.9605\n",
      "Epoch 18/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0950 - accuracy: 0.9586\n",
      "Epoch 18: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0947 - accuracy: 0.9587 - val_loss: 0.0891 - val_accuracy: 0.9605\n",
      "Epoch 19/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0985 - accuracy: 0.9564\n",
      "Epoch 19: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0985 - accuracy: 0.9564 - val_loss: 0.0928 - val_accuracy: 0.9612\n",
      "Epoch 20/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1011 - accuracy: 0.9547\n",
      "Epoch 20: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1011 - accuracy: 0.9547 - val_loss: 0.0888 - val_accuracy: 0.9633\n",
      "Epoch 21/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0986 - accuracy: 0.9583\n",
      "Epoch 21: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0986 - accuracy: 0.9582 - val_loss: 0.0977 - val_accuracy: 0.9558\n",
      "Epoch 22/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0915 - accuracy: 0.9588\n",
      "Epoch 22: val_loss did not improve from 0.05761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0916 - accuracy: 0.9587 - val_loss: 0.0905 - val_accuracy: 0.9585\n",
      "Epoch 23/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0965 - accuracy: 0.9566\n",
      "Epoch 23: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0969 - accuracy: 0.9567 - val_loss: 0.0776 - val_accuracy: 0.9721\n",
      "Epoch 24/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0989 - accuracy: 0.9571\n",
      "Epoch 24: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0991 - accuracy: 0.9570 - val_loss: 0.0871 - val_accuracy: 0.9667\n",
      "Epoch 25/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1016 - accuracy: 0.9553\n",
      "Epoch 25: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.1016 - accuracy: 0.9553 - val_loss: 0.0961 - val_accuracy: 0.9585\n",
      "Epoch 26/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1072 - accuracy: 0.9553\n",
      "Epoch 26: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1072 - accuracy: 0.9553 - val_loss: 0.0926 - val_accuracy: 0.9626\n",
      "Epoch 27/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0885 - accuracy: 0.9603\n",
      "Epoch 27: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0885 - accuracy: 0.9603 - val_loss: 0.0832 - val_accuracy: 0.9694\n",
      "Epoch 28/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0893 - accuracy: 0.9586\n",
      "Epoch 28: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0891 - accuracy: 0.9587 - val_loss: 0.0770 - val_accuracy: 0.9619\n",
      "Epoch 29/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0932 - accuracy: 0.9576\n",
      "Epoch 29: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0932 - accuracy: 0.9576 - val_loss: 0.0808 - val_accuracy: 0.9701\n",
      "Epoch 30/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0889 - accuracy: 0.9605\n",
      "Epoch 30: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0893 - accuracy: 0.9603 - val_loss: 0.1028 - val_accuracy: 0.9653\n",
      "Epoch 31/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0923 - accuracy: 0.9597\n",
      "Epoch 31: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0924 - accuracy: 0.9596 - val_loss: 0.0855 - val_accuracy: 0.9585\n",
      "Epoch 32/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0860 - accuracy: 0.9581\n",
      "Epoch 32: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0860 - accuracy: 0.9581 - val_loss: 0.0836 - val_accuracy: 0.9673\n",
      "Epoch 33/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9594\n",
      "Epoch 33: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0934 - accuracy: 0.9594 - val_loss: 0.1009 - val_accuracy: 0.9449\n",
      "Epoch 34/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.9548\n",
      "Epoch 34: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1317 - accuracy: 0.9548 - val_loss: 0.1697 - val_accuracy: 0.9544\n",
      "Epoch 35/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1050 - accuracy: 0.9593\n",
      "Epoch 35: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1050 - accuracy: 0.9593 - val_loss: 0.0873 - val_accuracy: 0.9653\n",
      "Epoch 36/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0915 - accuracy: 0.9565\n",
      "Epoch 36: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0915 - accuracy: 0.9565 - val_loss: 0.0877 - val_accuracy: 0.9619\n",
      "Epoch 37/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0906 - accuracy: 0.9592\n",
      "Epoch 37: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0907 - accuracy: 0.9591 - val_loss: 0.0872 - val_accuracy: 0.9701\n",
      "Epoch 38/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9579\n",
      "Epoch 38: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0944 - accuracy: 0.9579 - val_loss: 0.1065 - val_accuracy: 0.9585\n",
      "Epoch 39/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0930 - accuracy: 0.9576\n",
      "Epoch 39: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0934 - accuracy: 0.9574 - val_loss: 0.0811 - val_accuracy: 0.9701\n",
      "Epoch 40/100\n",
      "365/367 [============================>.] - ETA: 0s - loss: 0.0851 - accuracy: 0.9626\n",
      "Epoch 40: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0847 - accuracy: 0.9628 - val_loss: 0.0856 - val_accuracy: 0.9735\n",
      "Epoch 41/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0859 - accuracy: 0.9608\n",
      "Epoch 41: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0859 - accuracy: 0.9608 - val_loss: 0.0819 - val_accuracy: 0.9694\n",
      "Epoch 42/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9628\n",
      "Epoch 42: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0790 - accuracy: 0.9628 - val_loss: 0.0760 - val_accuracy: 0.9721\n",
      "Epoch 43/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0979 - accuracy: 0.9602\n",
      "Epoch 43: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0978 - accuracy: 0.9603 - val_loss: 0.0791 - val_accuracy: 0.9728\n",
      "Epoch 44/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.1003 - accuracy: 0.9576\n",
      "Epoch 44: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1002 - accuracy: 0.9574 - val_loss: 0.0907 - val_accuracy: 0.9646\n",
      "Epoch 45/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9593\n",
      "Epoch 45: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1029 - accuracy: 0.9593 - val_loss: 0.0862 - val_accuracy: 0.9714\n",
      "Epoch 46/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0906 - accuracy: 0.9599\n",
      "Epoch 46: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0906 - accuracy: 0.9599 - val_loss: 0.0767 - val_accuracy: 0.9687\n",
      "Epoch 47/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0888 - accuracy: 0.9616\n",
      "Epoch 47: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0888 - accuracy: 0.9616 - val_loss: 0.0786 - val_accuracy: 0.9701\n",
      "Epoch 48/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1017 - accuracy: 0.9593\n",
      "Epoch 48: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1017 - accuracy: 0.9593 - val_loss: 0.0805 - val_accuracy: 0.9673\n",
      "Epoch 49/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0892 - accuracy: 0.9603\n",
      "Epoch 49: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0891 - accuracy: 0.9605 - val_loss: 0.0819 - val_accuracy: 0.9626\n",
      "Epoch 50/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0872 - accuracy: 0.9636\n",
      "Epoch 50: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0871 - accuracy: 0.9635 - val_loss: 0.0803 - val_accuracy: 0.9633\n",
      "Epoch 51/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0906 - accuracy: 0.9620\n",
      "Epoch 51: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0906 - accuracy: 0.9620 - val_loss: 0.0893 - val_accuracy: 0.9660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.9649\n",
      "Epoch 52: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0808 - accuracy: 0.9649 - val_loss: 0.0735 - val_accuracy: 0.9667\n",
      "Epoch 53/100\n",
      "365/367 [============================>.] - ETA: 0s - loss: 0.0716 - accuracy: 0.9643\n",
      "Epoch 53: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0717 - accuracy: 0.9642 - val_loss: 0.0778 - val_accuracy: 0.9673\n",
      "Epoch 54/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0864 - accuracy: 0.9581\n",
      "Epoch 54: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0864 - accuracy: 0.9581 - val_loss: 0.0734 - val_accuracy: 0.9667\n",
      "Epoch 55/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0859 - accuracy: 0.9613\n",
      "Epoch 55: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0859 - accuracy: 0.9613 - val_loss: 0.0774 - val_accuracy: 0.9653\n",
      "Epoch 56/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.1072 - accuracy: 0.9581\n",
      "Epoch 56: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1065 - accuracy: 0.9584 - val_loss: 0.0853 - val_accuracy: 0.9612\n",
      "Epoch 57/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0867 - accuracy: 0.9631\n",
      "Epoch 57: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0867 - accuracy: 0.9630 - val_loss: 0.0896 - val_accuracy: 0.9605\n",
      "Epoch 58/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9637\n",
      "Epoch 58: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0794 - accuracy: 0.9637 - val_loss: 0.0809 - val_accuracy: 0.9639\n",
      "Epoch 59/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0959 - accuracy: 0.9586\n",
      "Epoch 59: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0956 - accuracy: 0.9587 - val_loss: 0.0814 - val_accuracy: 0.9660\n",
      "Epoch 60/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0843 - accuracy: 0.9648\n",
      "Epoch 60: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0842 - accuracy: 0.9649 - val_loss: 0.0757 - val_accuracy: 0.9755\n",
      "Epoch 61/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9636\n",
      "Epoch 61: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0806 - accuracy: 0.9635 - val_loss: 0.0720 - val_accuracy: 0.9701\n",
      "Epoch 62/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0838 - accuracy: 0.9610\n",
      "Epoch 62: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0837 - accuracy: 0.9610 - val_loss: 0.0782 - val_accuracy: 0.9667\n",
      "Epoch 63/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0818 - accuracy: 0.9649\n",
      "Epoch 63: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0818 - accuracy: 0.9649 - val_loss: 0.0802 - val_accuracy: 0.9667\n",
      "Epoch 64/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9612\n",
      "Epoch 64: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0869 - accuracy: 0.9613 - val_loss: 0.0728 - val_accuracy: 0.9653\n",
      "Epoch 65/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.9633\n",
      "Epoch 65: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0962 - accuracy: 0.9633 - val_loss: 0.0752 - val_accuracy: 0.9680\n",
      "Epoch 66/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9664\n",
      "Epoch 66: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0771 - accuracy: 0.9664 - val_loss: 0.0829 - val_accuracy: 0.9694\n",
      "Epoch 67/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9652\n",
      "Epoch 67: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0748 - accuracy: 0.9652 - val_loss: 0.0671 - val_accuracy: 0.9694\n",
      "Epoch 68/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0831 - accuracy: 0.9652\n",
      "Epoch 68: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0831 - accuracy: 0.9652 - val_loss: 0.0774 - val_accuracy: 0.9673\n",
      "Epoch 69/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0840 - accuracy: 0.9616\n",
      "Epoch 69: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0839 - accuracy: 0.9616 - val_loss: 0.0876 - val_accuracy: 0.9537\n",
      "Epoch 70/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0958 - accuracy: 0.9599\n",
      "Epoch 70: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0958 - accuracy: 0.9599 - val_loss: 0.0760 - val_accuracy: 0.9626\n",
      "Epoch 71/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9654\n",
      "Epoch 71: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0716 - accuracy: 0.9654 - val_loss: 0.0822 - val_accuracy: 0.9701\n",
      "Epoch 72/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0974 - accuracy: 0.9628\n",
      "Epoch 72: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0974 - accuracy: 0.9628 - val_loss: 0.0778 - val_accuracy: 0.9653\n",
      "Epoch 73/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0819 - accuracy: 0.9654\n",
      "Epoch 73: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0825 - accuracy: 0.9651 - val_loss: 0.0650 - val_accuracy: 0.9735\n",
      "Epoch 74/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9622\n",
      "Epoch 74: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0766 - accuracy: 0.9622 - val_loss: 0.0859 - val_accuracy: 0.9578\n",
      "Epoch 75/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0846 - accuracy: 0.9616\n",
      "Epoch 75: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0846 - accuracy: 0.9616 - val_loss: 0.0682 - val_accuracy: 0.9714\n",
      "Epoch 76/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9674\n",
      "Epoch 76: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0763 - accuracy: 0.9674 - val_loss: 0.0793 - val_accuracy: 0.9578\n",
      "Epoch 77/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0781 - accuracy: 0.9648\n",
      "Epoch 77: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 13ms/step - loss: 0.0784 - accuracy: 0.9645 - val_loss: 0.0673 - val_accuracy: 0.9782\n",
      "Epoch 78/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9630\n",
      "Epoch 78: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0794 - accuracy: 0.9630 - val_loss: 0.0701 - val_accuracy: 0.9707\n",
      "Epoch 79/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9598\n",
      "Epoch 79: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0821 - accuracy: 0.9598 - val_loss: 0.0691 - val_accuracy: 0.9714\n",
      "Epoch 80/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9640\n",
      "Epoch 80: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0764 - accuracy: 0.9640 - val_loss: 0.0682 - val_accuracy: 0.9735\n",
      "Epoch 81/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9651\n",
      "Epoch 81: val_loss did not improve from 0.05761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0762 - accuracy: 0.9651 - val_loss: 0.0686 - val_accuracy: 0.9748\n",
      "Epoch 82/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9659\n",
      "Epoch 82: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 5s 12ms/step - loss: 0.0715 - accuracy: 0.9659 - val_loss: 0.0782 - val_accuracy: 0.9646\n",
      "Epoch 83/100\n",
      "366/367 [============================>.] - ETA: 0s - loss: 0.0772 - accuracy: 0.9655\n",
      "Epoch 83: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0772 - accuracy: 0.9654 - val_loss: 0.0651 - val_accuracy: 0.9694\n",
      "Epoch 84/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9618\n",
      "Epoch 84: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0777 - accuracy: 0.9618 - val_loss: 0.0714 - val_accuracy: 0.9667\n",
      "Epoch 85/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0987 - accuracy: 0.9645\n",
      "Epoch 85: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0987 - accuracy: 0.9645 - val_loss: 0.0890 - val_accuracy: 0.9497\n",
      "Epoch 86/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9656\n",
      "Epoch 86: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0825 - accuracy: 0.9656 - val_loss: 0.0783 - val_accuracy: 0.9680\n",
      "Epoch 87/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0864 - accuracy: 0.9616\n",
      "Epoch 87: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0864 - accuracy: 0.9616 - val_loss: 0.0909 - val_accuracy: 0.9599\n",
      "Epoch 88/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9678\n",
      "Epoch 88: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0711 - accuracy: 0.9678 - val_loss: 0.0695 - val_accuracy: 0.9626\n",
      "Epoch 89/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0845 - accuracy: 0.9643\n",
      "Epoch 89: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0844 - accuracy: 0.9642 - val_loss: 0.0717 - val_accuracy: 0.9673\n",
      "Epoch 90/100\n",
      "363/367 [============================>.] - ETA: 0s - loss: 0.0886 - accuracy: 0.9640\n",
      "Epoch 90: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0881 - accuracy: 0.9644 - val_loss: 0.0863 - val_accuracy: 0.9680\n",
      "Epoch 91/100\n",
      "365/367 [============================>.] - ETA: 0s - loss: 0.0822 - accuracy: 0.9659\n",
      "Epoch 91: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0821 - accuracy: 0.9657 - val_loss: 0.0726 - val_accuracy: 0.9612\n",
      "Epoch 92/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9657\n",
      "Epoch 92: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0745 - accuracy: 0.9657 - val_loss: 0.0582 - val_accuracy: 0.9707\n",
      "Epoch 93/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9669\n",
      "Epoch 93: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0781 - accuracy: 0.9669 - val_loss: 0.0723 - val_accuracy: 0.9660\n",
      "Epoch 94/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9640\n",
      "Epoch 94: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0784 - accuracy: 0.9640 - val_loss: 0.0763 - val_accuracy: 0.9626\n",
      "Epoch 95/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0853 - accuracy: 0.9601\n",
      "Epoch 95: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0853 - accuracy: 0.9601 - val_loss: 0.0646 - val_accuracy: 0.9673\n",
      "Epoch 96/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9668\n",
      "Epoch 96: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0729 - accuracy: 0.9668 - val_loss: 0.0830 - val_accuracy: 0.9585\n",
      "Epoch 97/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.1009 - accuracy: 0.9620\n",
      "Epoch 97: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.1009 - accuracy: 0.9620 - val_loss: 0.0719 - val_accuracy: 0.9633\n",
      "Epoch 98/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9647\n",
      "Epoch 98: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0757 - accuracy: 0.9647 - val_loss: 0.0695 - val_accuracy: 0.9667\n",
      "Epoch 99/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9683\n",
      "Epoch 99: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0673 - accuracy: 0.9683 - val_loss: 0.0701 - val_accuracy: 0.9680\n",
      "Epoch 100/100\n",
      "367/367 [==============================] - ETA: 0s - loss: 0.0937 - accuracy: 0.9647\n",
      "Epoch 100: val_loss did not improve from 0.05761\n",
      "367/367 [==============================] - 4s 12ms/step - loss: 0.0937 - accuracy: 0.9647 - val_loss: 0.0723 - val_accuracy: 0.9667\n",
      "******************** Accuracy ******************** \n",
      "\n",
      "46/46 [==============================] - 0s 4ms/step\n",
      "                     0.9666666666666667                      \n",
      "\n",
      "******************** classification_report ******************** \n",
      "\n",
      "46/46 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       229\n",
      "           1       1.00      1.00      1.00       235\n",
      "           2       1.00      1.00      1.00       203\n",
      "           3       0.98      0.85      0.91       283\n",
      "           4       0.86      0.98      0.92       271\n",
      "           5       1.00      1.00      1.00       249\n",
      "\n",
      "    accuracy                           0.97      1470\n",
      "   macro avg       0.97      0.97      0.97      1470\n",
      "weighted avg       0.97      0.97      0.97      1470\n",
      "\n",
      "Score for fold 5: loss of 0.07234053313732147; accuracy of 96.66666388511658%\n"
     ]
    }
   ],
   "source": [
    "# Define per-fold score containers\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "filepath = \"./Saved_Models/CNN/best_CNN.hdf5\"\n",
    "model_checkpoint = ModelCheckpoint(filepath, verbose=1, save_best_only=True)\n",
    "\n",
    "for train, test in kfold.split(data_train, labels_train):\n",
    "    n_classes = labels_train[train].shape[1]\n",
    "    model_CNN = tuner_search.get_best_models(num_models=1)[0]\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "\n",
    "    steps_per_epoch = data_train[train].shape[0]//batch_size\n",
    "\n",
    "  #Early Stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='max', min_delta = 0.01, verbose=1, patience=5)\n",
    "\n",
    "  # Training the model\n",
    "    print('*'*20,'Fitting the model','*'*20,'\\n')\n",
    "    history = model_CNN.fit(data_train[train], labels_train[train],\n",
    "                           batch_size=batch_size,\n",
    "                           steps_per_epoch = steps_per_epoch,\n",
    "                           epochs=100,\n",
    "                           callbacks=[model_checkpoint],\n",
    "                           validation_data=(data_train[test],labels_train[test]))\n",
    "\n",
    "    labels_test_unary = np.argmax(labels_train[test], axis=1)\n",
    "    print('*'*20,'Accuracy','*'*20,'\\n')\n",
    "    acc=accuracy_score(labels_test_unary, int_labels(model_CNN,data_train[test]))\n",
    "    print(' '*20,acc,' '*20,'\\n')\n",
    "\n",
    "    print('*'*20,'classification_report','*'*20,'\\n')\n",
    "    print(classification_report(labels_test_unary, int_labels(model_CNN,data_train[test])))\n",
    "\n",
    "  # Generate generalization metrics\n",
    "    scores = model_CNN.evaluate(data_train[test], labels_train[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model_CNN.metrics_names[0]} of {scores[0]}; {model_CNN.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "  # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1f8c9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.0680188313126564 - Accuracy: 97.28075861930847%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.06750932335853577 - Accuracy: 96.60094976425171%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.0764959380030632 - Accuracy: 97.00680375099182%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.06135591119527817 - Accuracy: 97.68707752227783%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.07234053313732147 - Accuracy: 96.66666388511658%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 97.04845070838928 (+- 0.4023838752689346)\n",
      "> Loss: 0.069144107401371\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1f40a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the saved model file (HDF5 format)\n",
    "model_path = './Saved_Models/CNN/best_CNN.hdf5'\n",
    "\n",
    "# Load the model\n",
    "model_CNN = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b929df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Accuracy ******************** \n",
      "\n",
      "93/93 [==============================] - 1s 4ms/step\n",
      "                     0.9005768578215134                      \n",
      "\n",
      "******************** classification_report ******************** \n",
      "\n",
      "93/93 [==============================] - 0s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.96       496\n",
      "           1       0.90      0.96      0.93       471\n",
      "           2       0.91      0.97      0.94       420\n",
      "           3       0.79      0.76      0.77       491\n",
      "           4       0.82      0.82      0.82       532\n",
      "           5       0.99      0.97      0.98       537\n",
      "\n",
      "    accuracy                           0.90      2947\n",
      "   macro avg       0.90      0.90      0.90      2947\n",
      "weighted avg       0.90      0.90      0.90      2947\n",
      "\n",
      "******************** confusion_matrix ******************** \n",
      "\n",
      "93/93 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 38.0, 'Predicted')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_test_unary = np.argmax(labels_test, axis=1)\n",
    "print('*'*20,'Accuracy','*'*20,'\\n')\n",
    "acc=accuracy_score(labels_test_unary, int_labels(model_CNN,data_test))\n",
    "print(' '*20,acc,' '*20,'\\n')\n",
    "\n",
    "print('*'*20,'classification_report','*'*20,'\\n')\n",
    "print(classification_report(labels_test_unary, int_labels(model_CNN,data_test)))\n",
    "\n",
    "# Confusion matrix\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "print('*'*20,'confusion_matrix','*'*20,'\\n')\n",
    "cfm = pd.DataFrame(confusion_matrix(labels_test_unary, int_labels(model_CNN,data_test)))\n",
    "cfm = cfm / np.sum(cfm, axis=1)\n",
    "classes = ['WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS', 'SITTING', 'STANDING','LAYING']\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cfm, annot=True,cmap='Blues',fmt='.2%',xticklabels=classes, yticklabels=classes)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57df6cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
